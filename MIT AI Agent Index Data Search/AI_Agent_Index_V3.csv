Name,Website,Short description,Intended uses: What does the developer state that the system is intended for?,Date(s) deployed,Website,Legal name,Entity type,Country (location of developer or first author's first affiliation),Safety policies: What safety and/or responsibility policies are in place?,Backend model(s): What model(s) are used to power the system?,Public model specification: Is there formal documentation on the system’s intended uses and how it is designed to behave in them?,"Description of reasoning, planning, and memory implementation: How does the system 'think'?",Observation space: What is the system able to observe while 'thinking'?,Action space/tools: What direct actions can the system take?,User interface: How do users interact with the system?,Development cost and compute: What is known about the development costs?,Accessibility of components,,,,,Controls and guardrails: What notable methods are used to protect against harmful actions?,Monitoring and shutdown procedures: Are there any notable methods or protocols that allow for the system to be shut down if it is observed to behave harmfully?,Customer and usage restrictions: Are there know-your-customer measures or other restrictions on customers?,,,Safety: Have safety evaluations been conducted by the developers? What were the results?,Publicly reported external red-teaming or comparable auditing,,,Interoperability with other systems: What tools or integrations are available?,Usage statistics and patterns: Are there any notable observations about usage?,Other notes (if any)
,,,,,,,,,,,,,,,,,Weights: Are model parameters available?,Data: Is data available?,Code: Is code available?,Documentation: Is documentation available?,Scaffolding: Is system scaffolding available?,,,,"Notable benchmark evaluations (e.g., on SWE-Bench Verified)","Bespoke testing (e.g., demos)",,Personnel: Who were the red-teamers/auditors?,"Scope, scale, access, and methods: What access did red-teamers/auditors have and what actions did they take?",Findings: What did the red-teamers/auditors conclude?,,,
Alice,https://web.archive.org/web/20241228021419/https://www.11x.ai/,"11x develops ""digital workers"", or AI-agent designed to automate the entire sales development process [source]. They designed two agents: Alice, an AI SDR, designed to carry prospecting and engagement, and Mike, an AI Phone Agent, designed to carry call-based outreach.","Mike and Alice are autonomous AI sales workers for the sales sector. They handle tasks like voice calls, email campaigns, and LinkedIn outreach, executing complete workflows to optimize revenue generation [source]","October 28, 2023 [source]",https://web.archive.org/web/20241228021419/https://www.11x.ai/,11x Limited [source],Private Limited Company (UK),"Incorporated: UK (14393667) [source]. HQ: San Francisco, London [source]",Unknown,Unknown,None,"Its memory implementation tracks interactions and retains contextual information, enabling tailored follow-ups and maintaining communication across channels [source]","11x.ai observes prospect data (names, titles, companies), engagement metrics (opens, replies), channel activity (email, LinkedIn), historical interactions, and CRM updates. This enables it to analyze behavior, adapt outreach, and optimize lead generation strategies [source]","Automating prospecting, crafting and sending personalized messages, scheduling follow-ups, managing multi-channel outreach (email, LinkedIn), and booking meetings directly into calendars [source]","To configure Alice (AI SDR), users can define their ideal customer profile; set up their email domains and sequences; configure personalization parameters; establish outreach rules and limits. For Mike, (AI Phone Agent) users can upload their call scripts and talking points; configure call handling preferences; set up integration with their calendar; define lead qualification criteria [source]",Unknown,Unknown,Unknown,Closed source,Unavailable,Closed source,Unknown,Unknown,None,None,For Alice: [source]. For Mike: [source],"SOC2 Type II certification, fully complies with GDPR and CCPA [source]",Unknown,Unknown,Unknown,"Integrates with major CRM systems, email providers, calendar tools, phone systems, linkedIn (for social selling) [source]. It aims to integrate itself in one's workflow, but no full list is provided.",Some available on the website [source],
AIAgent.app,https://web.archive.org/web/20241223075645/https://aiagent.app/,An AI Agent which can be used for web-based tasks provided by the user [source],"Web based tasks, including market research, travel planning, etc.","First website archive from April 19, 2023 [source]",https://web.archive.org/web/20241223075645/https://aiagent.app/,"BIG CORP LIMITED, d/b/a AiAgent [source]",Private limited company [source],"Incorporation: England, UK (14438372) [source]. Registration: London, England [source]",Unknown,GPT 3.5 and GPT 4,None,"Decomposes task into smaller tasks by creating a to-do list; then, goes through iterative cycles of creating any more tasks that seem necessary, re-prioritizing the tasks, and executing the highest priority task [source]",The AI agent is able to navigate on the web (text inputs) [source],The agent can directly perform actions on the web [source],"Users create conversations with names, specify the language model they wish to use, and provide a goal to the AI agent [source]",Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Closed source,Closed source,Closed source,Unknown,None,None,None,None,None,None,None,None,None,32000+ users according to the website [source],
CodeAct 2.1,https://web.archive.org/web/20241112010059/https://www.all-hands.dev/blog/openhands-codeact-21-an-open-state-of-the-art-software-development-agent,"An open sources AI software developer agent built with the OpenHands, formally OpenDevin, framework as part of the CodeAct series of agents.","""OpenHands agents can do anything a human developer can: modify code, run commands, browse the web, call APIs, and yes—even copy code snippets from StackOverflow."" [source]","March 12, 2024 (as OpenDevin); November 1, 2024 [source]",https://web.archive.org/web/20241229190647/https://www.all-hands.dev/,"All Hands AI, Inc [source]",Corporation,"Incorporation: Delaware, USA (3591585) [source]",Unknown,"Various models can be used as a backend, but the developers use Claude Sonnet 3.5 by default [source]. The documentation has recommendations for what model to backend [source]",None,"OpenHands’ default CodeAct agent implements reasoning and planning through a multi-turn interaction framework where the agent processes observations from users and environment, plans using chain-of-thought, and executes Python code, bash commands, browser control, file editing actions, with memory maintained through context history and event stream, while learning from execution results and error feedback [source]","OpenHands’ observation space includes: natural language instructions and/or screenshots from users, bash/Python execution output and/or error messages, browser states (e.g., tab opened, web page content), and file editor outputs [source]","OpenHands’ action space includes: ability to execute arbitrary bash commands, and python code (including calling different APIs / libraries programmatically), interact with file editors, and interact with web browsers [source]","Users interact with the system through natural language and image (screenshot) queries and receive responses that includes text explanations, code executions, visuals, or Github pull requests.","Unknown , however the paper discusses the costs of evaluation runs [source]",N/A; backends various models,N/A; backends various models,Available [source],Documentation page [source] on Github [source] and pre-print [source],On Github [source] and pre-print [source],Agent is run in sandboxed environment. OpenHands also has a built-in security analyzer in collaboration with InvariantLab that monitor agent’s action [source],None,None,53% on SWE-bench verified [source],None,None,None,None,None,OpenHands with access to terminal and Python is able to use arbitrary software libraries and APIs.,OpenHands github has 41.7k stars and 4.6k forks [source],This paper uses OpenHands [source].
Amazon Q Developer,https://web.archive.org/web/20241231162225/https://aws.amazon.com/q/,A general purpose coding agent [source],General purpose software development tasks,"November 28, 2023 [source]",https://web.archive.org/web/20241231162225/https://aws.amazon.com/q/,"Amazon.com, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (AMAZON.COM INC (2620453)) [source]. HQ: Seattle [source]",Amazon's Responsible AI policy [source],"Amazon (Titan models), Anthropic, AI21 Labs, Cohere, Meta, Mistral AI, Stability AI [source]",Available [source],Unknown,"System can access user's input text, codebases, and data on AWS [source].","System is able to write code, natural language responses, use AWS resources, and leverage third-party integrations [source] [source].","Amazon Q can be used on code editors, like VS Code, or using command line interface [source]",Unknown,N/A; backends various models,N/A; backends various models,Closed source,Available [source],Closed source,"Bedrock has the option of ""returning control"" to the developer by allowing them to decide whether to execute actions in their own applications [source]. There are also relevant AWS security measures [source].",Users can access AWS CloudTrail and Amazon Cloudwatch to monitor Amazon Q [source].,None,55% on SWE-Bench verified [source],None,None,None,None,None,Documentation discusses integrations [source].,Unknown,
Claude 3.5 Sonnet (2024-10-22),https://web.archive.org/web/20241229024749/https://www.anthropic.com/news/developing-computer-use,An LLM that can use specialized tools for using a computer just from screenshots.,"The latest version of Claude 3.5 Sonnet [source] can, when run through the appropriate software setup, follow a user’s commands to move a cursor around their computer’s screen, click on relevant locations, and input information via a virtual keyboard, emulating the way people interact with their own computer [source]","October 22, 2024 [source]",https://web.archive.org/web/20241231211939/http://anthropic.com/,"Anthropic, PBC [source]",Benefit Corporation [source],"Incorporation: Delaware, USA (Anthropic, PBC (4860621)) [source]",Available [source],Claude 3.5 Sonnet Upgraded,Available [source],"N/A, it's an LLM and doesn't include scaffolding.",Images (of computer screens) and text.,"A large collection of computer tools, including mouse clicking, typing into GUIs, and reading/writing files using a CLI [source]",LLM used through the Anthropic API [source],Unknown,Closed source,Closed source,Closed source,Available [source],Some demo code for computer use is shared [source],Unknown,Anthropic recommends users run the agents on VMs instead of their PCs [source],None,53% on SWE-bench Verified [source],Demonstrations on Anthropic's YouTube channel [source],"Addendum to Claude 3 Model Card, concluding Claude 3.5 Sonnet  (2024-10-22) is an ASL-2 model [source].","Pre-deployment testing by US AISI and UK AISI, alongside independent assessment by METR [source]","Biological, cyber, software and AI development, and safeguard efficacy evaluations [source]",Results reported but no statements about safety or appropriateness to release [source],Anthropic's Model Context Protocol allows for Claude to be integrated flexibly with many apps [source].,7.1k stars on Anthropic Quickstarts which contains the computer use demo [source],
GPT Researcher,https://web.archive.org/web/20241219220517/https://gptr.dev/,GPT Researcher is an open source autonomous agent for comprehensive online research on a variety of tasks. Searches the web and assembles findings into a final report on a research question [source],"For conducting online research on a topic without hallucination. Intended for a variety of domains such as financial and legal assistance, academic research, and travel planning.","First alpha release July 9, 2023 [source]",https://web.archive.org/web/20241220000000/https://github.com/assafelovic/gpt-researcher,Assaf Elovic [source],Individual [source],Israel [source],None,Can use a wide variety of local and api-based LLMs such as OpenAI and Anthropic models [source],None,"The system uses a multiagent framework that splits performing research on a research question into multiple steps. One agent generates concrete specific questions to search for, while others perform the search using the Tabily engine and read articles in parallel. Finally, another agent aggregates the information and produces a report [source] [source]",Outputs of other agents in the multiagent system and results from a search engine (short processed snippets from HTML web pages) [source] [source],"It acts through search engines, scraping, embeddings and text generation [source] [source].","Primarily through the command line or a local web interface, where users can enter their desired research questions. However, users can also modify the open source code to introduce custom context or modify the underlying functionality of GPT researcher [source]",Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Available [source],Available [source],Available [source],None,None,None,None,Demo [source],None,None,None,None,"Search APIs such as Tavily, Bing, Google, Arxiv, PubMedCentral, etc. Also includes an ollama integration to use local models [source] [source]","Github repo has 15k stars and 2.1k forks. Stars have been increasing at an approximately linear rate of ~1,000 per month [source]",
Gru,https://gru.ai/,"Gru has four modes: ""1) Assistant Gru: Helps users solve standalone technical issues, which is now in public use; 2) Test Gru: Generates unit test code automatically; 3) Bug Fix Gru: Fixes bugs based on user issues automatically; and 4) Babel Gru: Assists in building end-to-end projects"" [source]",General-purpose coding and software development.,"Gru.ai founded in July, 2023 [source]. Product launch date is unclear.",https://web.archive.org/web/20241220064538/http://www.gru.ai/,"Babel Inc., Babelcloud Inc [source]",Corporation [source],"Incorporation: Delaware, USA (Babelcloud Inc. (2474686)) [source]",Unknown,Unknown,None,Unknown,Unknown,Unknown,Unknown,Unknown,Closed source,Closed source,Closed source,Unavailable,Closed source,Unknown,"Warning, suspension or termination of account if a user does not abide by the Terms of Service [source].",There is documentation on how the product can not be used [source].,57% on SWE-bench verified [source],Demo for Assistant Gru is available when logged in with a github account [source],None,None,None,None,None,Unknown,
Bardeen,https://web.archive.org/web/20241226202926/https://www.bardeen.ai/,"Workflow automation tool, LLM agent deployed locally with access to a computer","Meant to automate repetitive day-to-day work, such as writing emails, booking events. The key function is to automate repetitive tasks that span across multiple apps on a machine. ","Bardeen 2.0 announced January 17, 2024 [source]",https://web.archive.org/web/20241226202926/https://www.bardeen.ai/,"Bardeen, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (BARDEEN INC. 7927273) [source]",Unknown,Unknown,None,Unknown,Unknown,"Unclear, but Bardeen only seems to be constrained by the supported app integrations (i.e. by the apps locally installed on a given machine).",Through a chat interface with Bardeen.,Unknown,Unknown,Unknown,Closed source,Tutorials listed [source],Closed source,Unknown,Unknown,None,None,Demos and use cases listed on webpage [source].,None,None,None,None,"A plethora of possible integrations, including Slack, Amazon, Dropbox, OpenAI, Reddit, Google, among many others [source]",None,
Basepilot,https://web.archive.org/web/20241219234653/https://www.basepilot.com/,"Basepilot is a designed to automate repetitive, browser-based tasks resource-intensive back-office tasks across various industries such as logistics, insurance, finance, and real estate [source]","The platform uses agentic AI ""employees"" to handle workflows like data entry, form processing, invoicing, compliance tasks, and document processing, allowing human teams to focus on higher-value tasks that require more strategic input [source].",April 2024 [source],https://web.archive.org/web/20241219234653/https://www.basepilot.com/,"Basepilot, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (BASEPILOT, INC (2895569)) [source]",Unknown,Unknown,None,"Basepilot learns directly from user demonstrations within the browser. Users train it/construct their workflow by simply showing steps, allowing Basepilot to plan and execute based on observed actions [source] [source]. ","Operating as a Chrome extension, Basepilot observes and interacts within the browser tabs, enabling it to work across existing web tools without altering workflows. This observation space also lets users monitor each action in real-time [source]","Basepilot’s action space is the browser environment itself, particularly focused on tasks performed through the Chrome extension. Within this space, Basepilot interacts with elements on web pages and applications, including filling out forms, clicking buttons, navigating pages, extracting data, and inputting information across platforms that the user already employs [source].","Basepilot’s user interface is embedded directly in the Chrome extension, allowing users to interact with it from within their browser. Through this interface, users can teach Basepilot tasks by demonstrating actions directly on web pages. The UI likely includes simple controls for recording actions, monitoring task progress, and overseeing automated workflows in real time. Users can also interact with basepilot copilot via chat [source].",Unknown,N/A; backends various models,N/A; backends various models,Closed source,Unavailable,Closed source,Unknown,Unknown,None,Unknown,Basepilot AI Sales Assistant Demo [source],None,None,None,None,Chrome [source] and all insurance software stack [source],,
Sibyl System,https://web.archive.org/web/20240717072810/https://github.com/Ag2S1/Sibyl-System,Sibyl is a framework that transforms existing language models (i.e. GPT-4o) into agents that can complete tasks by using a web browser and Python interpreter [source],"Augment existing language models (i.e., GPT-4o), helping them to solve complex reasoning tasks [source]","Earliest GitHub commits from July 16, 2024 [source]",https://web.archive.org/web/20241204153959/http://www.baichuan-ai.com/home,"Beijing Baichuan Intelligent Technology Co., Ltd. (北京百川智能科技有限公司 [source])",Unknown,"Beijing, China [source]",Available [source],The default backend models are GPT-4 and GPT-4o [source],None,"Planning: Sibyl's tool planner processes a user's query and any associated step history to select appropriate tools. Reasoning: Sibyl's jury mechanism uses a ""multiagent debate format for self-critique and correction."" Memory: Sibyl's global workspace compresses and shares information between the agent's modules [source]","Sibyl operates in a workspace where it can observe outputs from a web browser and Python interpreter, along with its task memory [source]","Sibyl can execute code and search the internet. For a full breakdown of Sibyl's action space, see Appendix A of the technical report [source]",Code released in a GitHub repository without a user interface [source],Unknown,N/A; backends various models,N/A; backends various models,Available [source],Documentation on GitHub [source] and pre-print [source],Available [source],None,Depends on what is implemented in a specific configuration [source],None,34.55% average score on GAIA Benchmark [source],None,None,None,None,None,"By default, Sibyl interacts with only two external systems: a web browser and a Python interpreter. However, Sibyl is open-source and can be modified to integrate with other systems. According to its developers, Sibyl ""can be seamlessly integrated as a low-cost enhancement into existing frameworks, easily replacing the vanilla GPT-4 API."" [source]",The GitHub repository has 1 fork and 34 stars stars [source],
BlackBox Coding Agent,https://web.archive.org/web/20241231065444/https://www.blackbox.ai/,"""BLACKBOX AI is an AI coding assistant that helps developers by providing real-time code completion, documentation, and debugging suggestions. BLACKBOX AI is also integrated with a variety of developer tools, making it easy to use within your existing workflow."" [source]",General purpose coding and software engineering assistance.,"Deployed December 1, 2024 [source]",https://web.archive.org/web/20241231065444/https://www.blackbox.ai/,Course Connect Inc [source],Corporation [source],"Quebec, Canada [source]",None,Unknown,None,Unknown,Unknown  other than the codebase that it is working in [source],Unknown  other than code-writing suggestions and messages to the user. It is unclear whether the system can run code [source].,Through chats and commands in a VSCode extension [source],Unknown,Unknown,Unknown,Closed source,Available (but minimal) [source],Closed source,None,None,None,49% on SWE-Bench Lite [source],None,None,None,None,None,It is a VSCode extension [source],Has over 3 million installs [source],
Agent Workflow Memory,https://arxiv.org/pdf/2409.07429,AWM is a general-purpose web agent that includes a 'workflow memory' which is created by learning reusable routines for common tasks and integrating these workflows into a 'workflow memory' to guide future task-solving processes.,General purpose web tasks.,"Paper arXived September 11, 2023 [source]",https://arxiv.org/pdf/2409.07429,Carnegie Mellon University (et al.) [source],Academic Institutions [source],USA [source],None,They use GPT-4 (gpt-4-0613) [source],None,"AWM is a typical agent based on BrowserGym [source], but it is capable of flexibly choosing to use a workflow from its workflow memory (online mode) or not (offline mode). To add new workflows, ""the agent takes actions to solve given queries, induces workflows from successful ones, and integrates them into memory."" [source]",The system observes accessibility tree representations from each website [source],Webpage actions based on the page's accessibility tree. This includes post/submission actions. The system has the additional ability to query and write to the agent workflow memory.,A coding IDE and terminal.,Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Available [source],Unavailable (none beyond paper and GitHub),Available [source],None,None,None,WebArena (35.5) and Mind2Web (4.8) [source],None,None,None,None,None,None,Github repo has 227 stars and 20 forks [source],
Codebuff,https://web.archive.org/web/20241125222625/https://www.codebuff.com/,"Codebuff is a command-line interface (CLI) tool that automates code writing by acting as a coding agent within your terminal. It uses natural language commands to execute tasks such as installing packages, running tests, analyzing your codebase, and streamlining development workflows efficiently [source]","It's intended to assist in various coding tasks, including building features, writing unit tests, refactoring code, creating scripts, and providing advice, all while understanding the entire codebase context. The developers emphasize that Codebuff allows users to focus more on high-level architecture and design rather than implementation details, potentially increasing productivity and creativity in software development [source].","November 7, 2024 [source]",https://web.archive.org/web/20241125222625/https://www.codebuff.com/,"Codebuff, Inc [source]; Manicode (old name; [source])",Corporation,"Incorporation: Delaware, USA (Manicode Inc. 5231734) [source]",Unknown,"Uses Claude 3.5 Sonnet for coding and Haiku for file search. Also, uses a combination of Claude 3.5 Sonnet and GPT-4o-mini to rewrite files with an intended edit [source]",None,"Codebuff's works by first analyzing and caching the entire codebase structure using Claude Haiku 3.5. When processing user requests, it uses this cached context to quickly understand the relevant code. The system then employs Claude 3.5 Sonnet to select files and generate edits, while combining it with GPT-4o-mini for efficient code patching [source].","Codebuff observes the entire codebase structure, file contents, project-specific knowledge to make decisions and code [source]","Codebuff can parse a codebase, edit files, generate new code, and execute terminal commands [source]","Users interact with Codebuff through a command-line interface in their terminal, typing natural language instructions to request code changes, which Codebuff then processes and executes across the codebase [source]",Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Closed source,Available [source],Closed source,Unknown,The user can type 'undo' to remove edits [source],None,N/A; backends external model(s) via API,Demo [source],None,None,None,None,"Codebuff integrates with any development environment, including VSCode, Vim, Emacs, Replit, or plain text editors. It also autonomously utilizes existing tools, scripts, and packages (e.g., terminal commands, package managers like pip) without requiring explicit user approval [source]","Only detailed usage in a post when Codebuff was first launched -- ""The day after our Hacker News launch was our biggest yet, with >700 million tokens burned!"" [source]",
Aide,https://web.archive.org/web/20241224000855/https://aide.dev/blog/sota-bitter-lesson,A general-purpose software engineering assistant.,Rapid general-purpose coding and software development.,"June 28, 2024 [source]",https://web.archive.org/web/20241229171017/https://aide.dev/,"CODESTORY, INC [source]",Corporation [source],"Incorporation: Delaware, USA (CODESTORY INC (7556925)) [source]. HQ: California [source]",Unknown,Variable; SOTA on SWEbench-verified uses Sonnet 3.5 [source] [source],None,Uses parallel single-trajectory exploration and reward rubrics to evaluate the agent's tool actions [source].,"Agent has file system, code, and terminal access. It can view progress via the generated rewards [source].","The system accesses the following tools: List Files, Open File, Str_replace_editor, Attempt Completion, RipGrep search, Terminal Access [source]",Agent is available in their extension and will soon be available on the AIDE editor [source].,Unknown,N/A; backends various models,N/A; backends various models,Closed source,Unavailable,Closed source,Unknown,Unknown,None,62.2% on SWE-Bench verified beating the previous SOTA implementations [source],None,None,None,None,None,None,Unknown,
Devin,https://web.archive.org/web/20241217150025/https://www.cognition.ai/blog/introducing-devin,A general-purpose software engineering assistant.,Rapid general-purpose coding and software development.,Openly available [source],https://web.archive.org/web/20241224121630/https://www.cognition.ai/,"Cognition AI, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (COGNITION AI, INC. (2875752)) [source]. Business registration: California, USA Cognition AI, Inc. (6224891) [source]. HQ: New York [source]",Unknown,"Variable, including OpenAI o1 [source]",None,Unknown,"Devin operates in a workspace involving a browser, code editor, and shell and is able to observe the history of all [source]. ","Writing and executing code and queries in its environment browser, editor, and shell [source].","An application in which the user provide prompts; monitor the browser, code editor, and shell; and analyze outputs [source]. ",Unknown,N/A; backends various models,N/A; backends various models,Closed source,Available [source],Closed source,"""For complex tasks, Devin offers to wait for your confirmation"" [source]. Devin produces worklogs that users can view [source].",Unknown,None,13.86% on SWE-Bench [source],Demos [source] [source] [source].,None,None,None,None,"Devin integrates with Slack, GitHub, and Linear [source] [source]. ",Unknown,
ScribeAgent,https://web.archive.org/web/20250115055003/https://scribehow.com/library/scribe-agent,"ScribeAgent is a web agent trained to automate web workflows. Unlike most web agents, ScribeAgent is trained on Scribe's vast dataset of real step-by-step business processes, which Scribe collects as a part of its main product [source]","To complete complex web-based tasks, mostly in the business domain (e.g. working with CRM platforms, productivity tools like Notion, social platforms like Facebook, shopping sites like Amazon, and many others) [source]","Not yet deployed, paper released November 22, 2024 [source]",https://web.archive.org/web/20250109221924/https://scribehow.com/library/scribe-agent,"Colony Labs, Inc [source]",Corporation,"Incorporation: Delaware, USA (COLONY LABS, INC. 7498102) [source]",Unknown,Custom finetunes of Qwen2 [source],None,ScribeAgent is a single agent system without specialized planning or memory modules. ScribeAgent is trained to perform next-action prediction and rollouts are autoregressive [source],"The HTML DOM of the page in text format, with some elements removed through a simple hardcoded filter [source]",Take three types of web browser actions: 1) mouse click at element 2) type characters into an element 3) press hotkey combination (e.g. ctrl+c) [source],"There is no publicly available UI, but ScribeAgent takes a single prompt describing the desired task [source]",ScribeAgent- Small and Large can be finetuned using 8 H100 GPUs in four and ten days respectively [source],Closed source,Closed source,Available [source],Basic documentation on Github [source] and pre-print [source],Available [source],None,None,None,53% task success rate on WebArena [source],None,None,None,None,None,The WebArena benchmark code in the repository contains a minimal browser integration [source],None,
SWE Agent,https://composio.dev/swe-kit/,SWE-Kit is a framework for building Software Engineering agents using Composio’s tooling ecosystem. SWE-Kit comes with examples agents including their SWE agent [source].,General purpose software engineering.,"Announced November 26, 2024 [source]",https://web.archive.org/web/20241216224755/https://composio.dev/,Sampark Inc [source],Corporation [source],"Incorporation: Delaware, USA (SAMPARK INC, 7417658) [source]",None,"Variable, but backends Claude by default [source]",None,"There are three specialized agents for software engineering, code analysis, and code editing. The software engineering agent orchestrates the process [source].  ",It is able to work within the filesystem that it is deployed in [source].,SWE agents are able to perform basic actions in its fileystem including running shell commands which can include running arbitrary code [source].,A coding IDE and terminal.,Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Available [source],Available [source],Available [source],"""The SWE-agent runs in Docker by default for security and isolation. This sandboxes the agent's operations, protecting against unintended consequences of arbitrary code execution."" [source]",None,None,48.6% on SWE-Bench Verified [source],None,None,None,None,None,None,Composio has 4.3k forks and 14k stars [source],
Genie,https://web.archive.org/web/20240925010719/https://cosine.sh/genie,An AI software engineer,Emulation of human software engineers [source],"August 12, 2024 [source]",https://web.archive.org/web/20241123055437/https://cosine.sh/,BUILDT AI LIMITED (UK) [source]. Buildt Inc. (US) [source],Private limited Company (UK) [source]. Corporation (US) [source]. US Corporation controls over 75 percent of the shares and voting rights of the UK Private limited Company [source],"Incorporation: Delaware, USA (BUILDT INC. 7156765) [source]",Unknown,Variable models which they fine-tuned on their data [source],None,"Proprietary [source], however, the system is trained on a dataset that resembles a software engineer's workflow [source]. ","GitHub access, workspace in which it can plan, write code, and run tests.","Write and execute code, debugging tools, GitHub access [source]","Can prompt with a freeform prompt, ticket, or link a GitHub issue; monitor the agent finding info, planning, writing code, and running tests [source]",Unknown,Closed source,"Closed source; however, Cosine claims that the data ""perfectly emulates the cognitive processes, logic and workflow of human engineers. Our proprietary techniques generates data that represents perfect information lineage, incremental knowledge discovery, and step by step decision making"" [source]",Closed source,Available [source],Closed source,Unknown,Unknown,Currently limited to select users who can sign up via waitlist [source],30.08% on SWE-Bench [source],Demos [source],None,None,None,None,GitHub [source],Available to select users; anyone can register for the waitlist [source],
Cursor Agent,https://web.archive.org/web/20250101102215/https://www.cursor.com/,Cursor is a coding assistant designed to help draft and review code [source],General purpose code development [source],"March 14, 2023, however a coding ""agent"" was not introduced within cursor until November 24, 2024 [source]",https://www.cursor.com/,"Anysphere, Inc [source]",Corporation,"Incorporation: Delaware, USA (Anysphere, Inc. 6524309) [source]",Unknown,Variable including GPT and Claude models [source],None,"Cursor agent works in a ""shadow"" workspace in which is can iteratively develop draft code before suggesting to the user [source] [source]",Cursor can see all files in a codebase [source] and is able to browse the web [source],"Cursor is designed to only suggest pieces of code to the user. Cursor can run code in its ""shadow workspace"", but it is sandboxed [source]",Cursor is a coding IDE created as a fork of VSCode [source],Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Closed source,Available [source] [source],Closed source,Users can define specific rules for cursor [source],"Cursor can only propose actions to a user for them to accept or reject. Cursor can run code in the shadow workspace, but it is sandboxed [source]",None,None,Demos [source],None,None,None,None,Cursor is a coding IDE created as a fork of VSCode [source],"Unclear, but there is an active forum [source]",
Lucy,https://web.archive.org/web/20241223121501/https://www.cykel.ai/,"Digital worker platform for recruitment, sales and research agents [source]","Automating repetitive tasks/""workflows"" associated with recruitment, sales and research teams through their digital workers [source]","The flagship agent named ""Lucy"" was announced December 5, 2024 [source]",https://web.archive.org/web/20241229130924/https://h2o.ai/,Cykel AI PLC [source],Public limited company (UK) [source],"Incorporation: England, UK (11155663) [source]. Registration: England, Wales [source]",Unknown,Unknown,None,Unknown,"Digital workers operate within the existing software environment of a company. Nonetheless, the company has to set permissions, define workflows, and can monitor all activities in real-time. Digital workers can adapt themselves to the preferences over time. Operations can be scaled up or down instantly and workflows modified whenever needed [source]","Agents are designed to interact with all the software used by the company (extensive integration of over a hundred different tools), modify files, send emails/contact clients through their CRM and supposedly, interact with any available API.. [source]",There is a dashboard to configure the agents and view their tasks [source],Unknown,Unknown,Unknown,Closed source,Closed source,Closed source,"The company/client has to set permissions, define workflows, and can monitor all activities in real-time. Operations can be scaled up or down instantly and workflows modified whenever needed [source]",Unknown,None,None,Demos [source],None,None,None,None,They provide a sample of the most common integrations they offer and say they can be contacted for some more specific ones [source],None,
DeepSeek-V3,https://web.archive.org/web/20241227082440/https://www.deepseek.com/,Deepseek-v3 is an open-source general purpose reasoning agent.,General purpose uses including chat.,"Announced December 26, 2024 [source]",https://web.archive.org/web/20241227082440/https://www.deepseek.com/,DeepSeek [source],Corporation [source],China [source],Unknown,DeepSeek-V3 [source],None,"""We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3."" [source] ",Textual inputs from users. The DeepSpeek web chat app cannot search the web [source],Natural language.,"The DeepSpeek web chat app is a simple chat interface [source]. However, since the system is open-source, developers can interact through it with coding tools and build custom interfaces/scaffolding [source] ",Reported to take 6 million in training compute cost [source],Available [source],Closed-source,Available [source],Available [source] in addition to a technical report [source],Available [source],None,None,None,Various reported [source] [source],None,None,None,None,None,None,Unknown,
MetaGPT,https://web.archive.org/web/20241221061253/https://www.deepwisdom.ai/,A meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. Acts as an AI software company developing diverse range of software solutions.,MetaGPT adopts an assembly line paradigm to assign different roles to GPTs to form a collaborative software entity for solving complex tasks by efficiently breaking them down into subtasks involving many agents working together [source],"June 30, 2023 [source]",https://www.deepwisdom.ai/metagpt,MetaGPT LLC [source] (see Terms of Service),Corporation,"Incorporation: Delaware, USA (7606285) [source]",Unknown,Supports a variety of backend models [source],None,"MetaGPT assigns specific roles (e.g. Engineer, Tester etc.) to each agent and initialize specialized skills and context for each role. A structured communication protocol is used between agents for collaboration, where all agents publish messages in common pool and agents can subscribe to receive messages during task solving process. The system employs iterative programming via self-correcting executable feedback (e.g Unit Test Generator). Memory is role specific and consists of list of messages, which contain all the necessary information, context and observations. Everything is modeled by roles, where new roles can be added for agents or humans can take one of the roles [source]","In MetaGPT, each agent observes messages from other agents and any context about the task it is solving specific to the role, feedback from previous projects and iterative feedback from current project [source]","Each agent can directly execute tasks specific to their role e.g. writeDesign, writeCode, writeCodeReview, writePRD, writeTasks, publish messages, subscribe to messages (new actions can be added specific to role)","MetaGPT takes a one line requirement as input and outputs user stories / competitive analysis / requirements / data structures / APIs / documents, etc.",Unknown,N/A; backends various models,N/A; backends various models,Available [source],"Documentation page [source] on Github [source], publication [source] and pre-prints [source] [source]",On Github [source] and publication [source],None,None,None,"87.7% on MBPP (Pass @1) and 85.9% on HumanEval [source], 46.67% on SWE-lite [source]; some more benchmarks for specific components, Data Interpreter [source] and AFlow [source].",Demo [source],None,None,None,None,MetaGPT allows to create your own tools and supports some tool usage by specific role (e.g. web search tools) but does not support UI or front-end based tools [source] [source],MetaGPT has 46.7k stars and 5.45k forks [source]; publication has ~500 citations,
Dosu,https://web.archive.org/web/20241205190358/https://dosu.dev/,"Dosu is an AI-powered teammate that automates issue responses, bug triage, and documentation updates. [source]","Dosu's features include Auto-Labeling, which organizes tasks and tickets by automatically applying relevant labels based on context and past activity; Issue Triage + Q&A, which leverages a knowledge base and agent workflows to provide expert guidance directly in GitHub or Slack discussions; and Changelog Generation (currently in beta), which automates the creation of detailed changelogs to simplify team communication [source]","June 24, 2023 (first commit) [source]",https://web.archive.org/web/20241205190358/https://dosu.dev/,"Dosu, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (DOSU, INC. 7438522) [source]. HQ: San Francisco, CA, USA [source]",Unknown,"""We use many different models, mostly hosted ones from OpenAI, Anthropic, and others. We use different models at different steps in our pipeline. In the future, we hope to use more and more open-source models as the company matures."" [source]",None,"Dosu uses continual in-context learning to dynamically adapt its reasoning and planning by leveraging user-provided corrections stored in an example store. At inference, it retrieves the most relevant examples for a task, enabling it to “think” and adjust based on evolving organizational workflows and specific needs [source]","Dosu’s observation space consists of the data available from its configured data sources within a workspace, including target-specific details and integrated knowledge bases. For example, a workspace could be a GitHub repository, Jira project, or Slack channel, while data sources could include the repository files, commit history, issue trackers, or message archives from these platforms [source]","Dosu has internal tools to search code, commits, tickets, and documentation and user-facing tools to comment on threads, label tickets, and close/open tickets. [source]","Users interact with Dosu as a bot through its integrations with platforms like GitHub, Slack, and Linear using natural language [source]",Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Closed source,Available [source],Closed source,Unknown,"Dosu’s Response Previews feature allows users to observe its commenting behavior in a non-intrusive way, enabling them to see hypothetical responses Dosu would generate without affecting live threads [source]",None,N/A; backends external model(s) via API,Free demo available upon request [source],None,None,None,None,"Integrations with platforms including GitHub, Slack, and Linear.  [source]",None,
gptme,https://web.archive.org/web/20241226055506/https://github.com/ErikBjare/gptme,"“Personal AI assistant in your terminal, with tools so it can: Use the terminal, run code, edit files, browse the web, use vision, and much more; Assists in all kinds of knowledge-work, especially programming, from a simple but powerful CLI [command-line interface].” [source]","Use Cases. (1) Write and run code faster with AI assistance. (2) Get the right command using natural language. (3) Process and analyze data directly in your terminal. (4) Experiment with new technologies or codebases hands-on. (5) Experiment with agents and tools in a local environment [e.g., give the assistant access to a full desktop, allowing it to interact with GUI applications — or — create your own agent with persistence using gptme-agent-template] [source]","Main announcement on September 5, 2023 [source].",https://web.archive.org/web/20241128133207/https://gptme.org/,Erik Bjäreholt [source],Individual [source],Sweden [source],None,"Variable, defaulting to GPT-4o [source]",None,gptme implements a simple loop for allowing the model to plan and act using default prompts that are available in documentation [source],"Gptme-agent can access terminal outputs, web pages, screenshots of your desktop, self-correction feedback, GUI applications, and files from a codebase [source]",Examples of tools available to gptme include: Shell - Execute shell commands; Python - Run Python code; Browser - Browse and interact with web content; Vision - Process and analyze images [source],"Besides the command-line interface, gptme can be used through a web-based interface [source] and as a GitHub bot [source].",Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Available [source],Available [source],Available [source],Unknown,Unknown,None,None,Demos [source] and bespoke benchmarks [source].,None,None,None,None,"As an open-source model, gptme is relatively interoperable with other systems. By default, it can be integrated with some editors, such as Vim [source]",Unknown,
Code Droid,https://web.archive.org/web/20240905220438/https://www.factory.ai/droids,Agent for software development [source],General-purpose coding and software development,"Code Droid documentation was posted June 18, 2024",https://web.archive.org/web/20241224152919/https://www.factory.ai/,The San Francisco AI Factory Inc [source],Corporation [source],USA [source],Unknown,"Variable, including models from Anthropic and OpenAI [source]",None,"Only high level details are given. Droid can decompose tasks, ""simulate decisions, perform self-criticism, and reflect on real and imagined decisions"". Droid uses ""HyperCode to construct a multi-resolution representation of a given engineering system"" and ByteRank for retrieving task-relevant information [source].","From the codebase, Droid can ""autonomously construct explicit (graph) and implicit (latent space similarity) relationships within low-level data and extract insights about these relationships at different levels of abstraction"" [source].",Droid has access to developer tools and environments (intended to be similar to what is available to human developers) [source].,Unknown,Unknown,N/A; backends various models,N/A; backends various models,Closed source,"Unavailable, but they have a technical report [source]",Closed source,"1. ""Code Droid operates within a strictly defined, sandboxed environment that isolates its operational scope from main development environments."" 2. ""Droids log and report the reasoning behind all of their actions."" 3. ""DroidShield performs real-time static code analysis to detect potential security vulnerabilities, bugs, or intellectual property breaches before they are committed to code"" [source].",Unknown,None,19.27% on SWE-bench Full and 31.67% on SWE-bench Lite [source],None,None,None,None,None,None,Unknown,
Astra,https://web.archive.org/web/20241219095324/https://deepmind.google/technologies/project-astra/,Phone-based multimodal assistant with tool use.,Prototype of a universal AI assistant [source],"Not deployed, ""trusted tested waitlist"" debut Dec 11 [source]",https://web.archive.org/web/20241231224948/https://deepmind.google/,Google LLC [source],LLC,"Incorporation: Delaware, USA (GOOGLE LLC 3582691) [source]",Several relevant commitments [source] [source] [source],Gemini [source],None,"10-minute memory, remembers past interactions [source]","Live speech, images, video, text [source]","Uses Google Search, Maps and Lens for conducting research [source]",Live video feed with voice and chat interface in demo [source],Unknown,Closed source,Closed source,Closed source,Unavailable,Closed source,Unknown,Unknown,Currently behind a waitlist,None,Demos [source],None,None,None,None,"Uses Google Search, Maps and Lens for conducting research [source]",None,
Jules,https://web.archive.org/web/20241230231523/https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/,AI coding / software engineering agent based on Google's Gemini 2 model.,Completing complex coding / software engineering tasks [source],"Not deployed, ""We’re making Jules available for a select group of trusted testers today, and we'll make it available for other interested developers in early 2025."" Debut Dec 11 [source]",https://web.archive.org/web/20241231224948/https://deepmind.google/,Google LLC [source],LLC,"Incorporation: Delaware, USA (GOOGLE LLC 3582691) [source]",Several relevant commitments [source] [source] [source],Gemini 2.0 [source],None,Unknown,Text. Possibly other modalities also (as Gemini 2 is multimodal) [source],"Modifying files, and preparing pull requests [source]. ",Unclear. This [source] contains a short video showing a GUI where the user describes the task they want Jules to complete and it returns code and diffs to files.,Unknown,Closed source,Closed source,Closed source,Unavailable,Closed source,Unknown,Unknown,Currently behind a waitlist,52.2% on SWE-bench Verified [source],None,None,None,None,None,Can prepare pull requests suggesting GitHub integration [source],None,
Mariner,https://web.archive.org/web/20241228145736/https://deepmind.google/technologies/project-mariner/,"""A research prototype exploring the future of human-agent interaction, starting with your browser"" [source]",Automate web-based tasks which require navigating websites,"Not deployed, ""trusted tested waitlist"" debut Dec 11 [source]",https://web.archive.org/web/20241231224948/https://deepmind.google/,Google LLC [source],LLC,"Incorporation: Delaware, USA (GOOGLE LLC 3582691) [source]",Several relevant commitments [source] [source] [source],Gemini 2.0 [source],None,Existence of memory and visible reasoning steps as seen in demo [source],https://web.archive.org/web/20241228145736/https://deepmind.google/technologies/project-mariner/,Interacting with websites and sending text messages to the user [source],Chat sidebar in browser for text or voice commands [source],Unknown,Closed source,Closed source,Closed source,Unavailable,Closed source,Unknown,Unknown,Currently behind a waitlist,"84% on ScreenSpot, 90.5% on WebVoyager [source]",Demos [source],None,None,None,None,Google Chrome,None,
h2oGPTe,https://web.archive.org/web/20241227125638/https://h2o.ai/platform/enterprise-h2ogpte/,"The industry’s first multi-agent Generative AI platform to bring together the strengths of Generative AI and Predictive AI with airgapped, on-premise deployment options [source]",Generative AI platform for businesses [source],"Multiple versions exist, some date back to Dec 12 2023 [source]",https://web.archive.org/web/20250102014554/https://h2o.ai/,"H2O.AI, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (H2O.AI, INC. 5060671) [source]",Unknown,"The primary model is Claude 3.5 Sonnet, but it can use other models [source]",None,"Next steps are determined by ""intelligent model routing"" [source]. A variety of architectural modules are used [source].","Text, audio, and images [source]","Depending on the use case, it can write code, create files, and chat with the user [source]",Primarily a chat interface [source],Unknown,Unknown,Unknown,Closed source,Available [source],Partially available [source],"Supports customizable guardrails against prompts, types of responses, regex patterns, presidio labels, PII labels [source]",Unknown,None,65% on GAIA [source],Guide [source],None,None,None,None,Integrates with H2O feature store [source],Customer case studies available for the company [source],
Runner H,https://web.archive.org/web/20241224155629/https://www.hcompany.ai/blog/introducing-h,"Runner H is the flagship model for 'H Company's' Ai Agent development platform, ""Studio."" ",Runner H is a general-purpose agent designed to automate tasks on a filesystem and browser.,"Announced November 19, 2023. Currently in private beta [source]",https://www.hcompany.ai/blog/introducing-h,H.AI SAS [source],Société par actions simplifiée (~US LLC) [source],Incorporation: France (927528471) [source],Unknown,"Their agents are based on in-house models: H-VLM and H-LLM. Their H-VLM is a 3B parameter vision language model trained to extract information from and interact with GUIs and images. Meanwhile, they also introduce that H-LLM family of language models, one of which is 2B parameters [source].",None,Unknown,"H-VLM is able to observe images, diagrams, and GUIs [source]",Runner H can interact with a browser. It is not clear what it can do beyond that.,Chat [source],Unknown,Closed source,Closed source,Closed source,Unavailable,Closed source,Unknown,Unknown,Currently in a private beta.,"67% on WebVoyage, 80% on Screenspot, and 65% average across several code and function calling benchmarks [source]",Demos [source],None,None,None,None,H-VLM is designed to integrate dynamically with web user interfaces.,Unknown,
Neo,https://web.archive.org/web/20241120161541/https://heyneo.so/,"Neo is an autonomous AI engineer. It's a multi-agent system capable of solving complex ML engineering problems by automating the machine learning workflow. The developers write ""It's like having a kaggle master/expert in your team"" [source] . ","""Automating the entire machine learning workflow."" [source]","It is used and tested internally and was introduced on the company's blog on November 15, 2024 [source]. However there is currently a waitlist for a private beta. ",https://web.archive.org/web/20241120161541/https://heyneo.so/,HeyNeo [source],Unknown,USA [source],Unknown,Unknown,None,"""Given a specific objective, NEO initiates a comprehensive workflow to reach its goal. NEO utilizes a structured, multi-step approach to achieve its objectives by breaking down complex problems into manageable components. This approach involves a continuous loop of planning, coding, executing, and debugging — ensuring thorough refinement at each stage. As NEO progresses through these steps, it adapts and iterates until optimal results are achieved. Once developers approve NEO's output, the workflow deploys in seconds. NEO simplifies all the intricacies discussed above for Machine Learning Engineers."" [source]","Neo can see chat, the filesystem, and can browse the web [source]",Neo can run commands in a terminal and produce output for the user to view [source]. NEO uses containerized GPU/CPU sandboxes to perform code executions.,"Chat. The user can also see an artifact viewer, terminal, monitor, browser, and file explorer, to oversee Neo's actions [source]",Unknown,Unknown,Unknown,Closed source,Unavailable,Closed source,Unknown,"Based on video demos, the user can monitor Neo's activity and intervene if needed [source]. ",There is currently a waitlist for a private Beta [source],MLE Bench (26%) [source],Demos [source],None,None,None,None,None,Unknown,
Grit Agent,https://perma.cc/9M7E-QE6L,https://perma.cc/9M7E-QE6L,For autonomously making large scale changes to codebases such as code migrations (from one API/framework to another) or refactors [source] [source],Unknown,https://perma.cc/9M7E-QE6L,"Iuvo AI, Inc [source]",Corporation [source] [source],"Incorporation: Delaware, USA (IUVO AI, INC. 6427074) [source] [source]",Unknown,Unknown,None,"The agent works in three stages: 1. Planning. Combines static analysis with LLMs to produce an index of the codebase. Then creates a rough plan for how to change the code. 2. Generation. Applies a series of transformations to the code using AI + Grit DSL. Queries online documentation to inform this step. 3. Refinement. Creates a pull request and gets feedback from unit tests, CI tools, an automated reviewer, as well as human reviews. Then rewrites code based on feedback [source]","The raw code, an index of the code (details dependencies, functionality of each file, and class hierarchies), online documentation, and unit test and reviewer feedback [source]","Rewrite codebases in a sandbox, search for and read online documentation, submit pull requests, and run unit tests [source]",Through a web interface called Grit Studio [source],Unknown,Closed source,Closed source,Closed source,Available [source],Closed source,Unknown,Unknown,Must schedule call with Grit to obtain access to paid tier [source],None,"Minimal, but some demo videos available on the internet of predefined workflows being executed [source]",None,None,None,None,"Integrates with github, GritQL code transformation language, internet search tool for documentation, and python interpreter [source]","Not many, but a few customers (like LangChain) on Twitter speak about the Grit system [source] [source].",
Claude Coder,https://web.archive.org/web/20241211083422/https://www.kodu.ai/,Claude Coder is a VSCode Extension that can assist with general purpose code development and software engineering tasks.,General purpose coding and software engineering assistance.,"December 28, 2024 [source]",https://www.kodu.ai/,Kodu.ai [source],Unknown,Japan [source],None,Claude 3.5 Sonnet [source],None,Unknown,Unknown  other than the codebase that it is working in [source],Unknown  other than code-writing suggestions and messages to the user. It is unclear whether the system can run code [source].,Through chats and commands in a VSCode Extension [source],Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Available [source],None,Available [source],None,The user is able to observe a live log of the agent's activity [source],None,44.67% on SWE-Bench Lite [source],Demos [source],None,None,None,None,It is a VSCode extension [source],Has over 11k installs [source],
LinkedIn Talent Agents,https://web.archive.org/web/20241212094850/https://business.linkedin.com/talent-solutions/hiring-assistant,Talent agents automate tasks related to the hiring process on LinkedIn.,"Tasks related to finding and hiring candidates, including: creating job postings, creating search filters for candidates, managing a candidate pipeline, drafting messages, scheduling interviews [source]","October 29, 2024 [source]",https://business.linkedin.com/,LinkedIn Corporation [source],C Corp [source],"Incorporation: Delaware, USA [source]. HQ: Sunnyvale, CA, USA (id)",LinkedIn has Responsible AI Principles which they acknowledge as adhering to those of Microsoft [source],"Unclear, although likely an OpenAI model, since this is used for LinkedIn's other AI offerings [source]",None,"An ""agent orchestration layer"" that allows the AI system to engage with the user and tools on the LinkedIn platform (e.g. search and messaging) [source]","LinkedIn data (e.g. profiles), descriptions provided by user [source]",System is able to browse LinkedIn and present recommendations to the user [source],"The interfaces are embedded throughout the platform (i.e. when users are engaged in hiring, the agent appears accessible); interactions occur through chat [source]",Unknown,N/A; backends external model(s) via API,"Has access to ""unique platform insights of over 1 billion members, 68M companies, and 41K skills"" [source]",Closed source,Available [source],Closed source,Unknown,Unknown,Currently only available to select corporate customers and for internal use [source]; anticipated broader rollout in late 2025 [source],Unknown,Demos [source] [source],None,None,None,None,Integrated with LinkedIn,"Limited customers include AMS, Canva, Siemens, and Zurich Insurance [source]",LinkedIn is also owned by Microsoft/operates as a subsidiary
Magentic One,https://www.microsoft.com/en-us/research/publication/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/,A multiagent system introduced by Microsoft with general capabilities.,"It is used for ""ad-hoc, open-ended tasks such as browsing the web and interacting with web-based applications, handling files, and writing and executing Python code"" [source]","Announced November 4, 2023 [source]",https://web.archive.org/web/20241231232226/https://www.microsoft.com/en-us/,Microsoft Corporation [source],Corporation [source],"Incorporation: Washington, USA (MICROSOFT CORPORATION (2357303)) [source]. Registration: Delaware, USA. HQ: Washington, USA [source]",Model evaluations and red teaming; model reporting and information sharing; security controls [source]. Microsoft's safety policies are described online [source],"The default model used is gpt-4o-2024-05-13, but they also experiment with using Openai o1 [source].",Available [source],"The system contains multiple subagents that work together to solve problems. Things are controlled at a high level by the ""Orchestrator"" agent and executed by the ""WebSurfer,"" FileSurfer,"" ""Coder,"" and ""ComputerTerminal"" agents [source].",It has full access to a filesystem and web browser.,"It is able to surf (including posting) on the web, execute file system commands, and write/execute code. ",Users can configure and experiment with it using the AutoGen package [source],Unknown,N/A; backends various models,N/A; backends various models,Available on GitHub as part of Microsoft's AutoGen project [source].,"Available on GitHub [source], see also the technical report [source]",Available [source],"The developers recommend using containers, virtual environments, log monitoring, human oversight, access limitations, and data safeguards. ",Logs are kept while the system runs.,None,"GAIA (38%), AssistantBench (27.7), and WebArena (32.8%) [source]",None,"They report on ad-hoc evaluations of failures and safety concerns in the technical report [source]. The developers claim: ""We performed testing for Responsible AI harm e.g., cross-domain prompt injection and all tests returned the expected results with no signs of jailbreak"" [source]",None,None,None,It was not explicitly designed to interoperate with any particular systems other than the web browser and filesystem. But it presumably could integrate with others with little configuration.,Microsoft AutoGen has 36.9k stars and 5.3k forks [source],
Agent Q,https://web.archive.org/web/20241217141020/https://www.multion.ai/,"""AI agents that can fully complete tasks in any web environment"" [source]",Completion of web-based tasks [source],"Agent Q announced October 13, 2024 [source]",https://web.archive.org/web/20241217141020/https://www.multion.ai/,MULTION INC [source],Corporation [source],"Delaware, USA (MULTION INC; (6910022)) [source]. HQ: Palo Alto, CA, USA [source]",Unknown,Unknown,None,"""Combines guided Monte Carlo Tree Search (MCTS) search [over web pages to guide agent exploration] with a self-critique mechanism.. [and divides] Agent output format into an overall step-by-step plan, thought, a command, and a status code."" [source]","For Agent 2.0: ""system prompt, execution history, the current observation as a DOM representation, and the user query containing the goal"" [source]",Chrome extension that has full control of the browser [source],"Display that allows user to input web prompts in natural language or select from predefined options (eg book a meeting, make a tweet, call an uber) [source]; shows both the screen that the agent is controlling and the reasoning/thought in a chat window; option to stop or restart",Unknown,Closed source,Closed source,Closed source,Available [source],Closed source,Unknown,Unknown,None,Unknown,External demo [source],None,None,None,None,Chrome extension [source],Early access waitlist available [source],
Allice,https://web.archive.org/web/20241123091858/https://github.com/myshell-ai/AIlice,"AIlice is an open-source, fully autonomous AI assistant designed to performs complex tasks using LLMs. It supports multimodal and voice interactions and can dynamically expand its capabilities through modular configurations. [source]","Research, coding, system management, literature reviews and ""complex hybrid tasks""  [source]","June 22, 2024 [source]",https://web.archive.org/web/20241123091858/https://github.com/myshell-ai/AIlice,Myshell Limited [source] [source],Unknown,"California, USA [source]",Unknown,"Commercial LLMs like OpenAI’s GPT-4 and Anthropic’s Claude, as well as open-source models such as Qwen-2-72b-instruct, Mixtral, Meta LLaMA, and integrations with platforms like Ollama and LM Studio [source]",None,"Decomposes tasks into subtasks managed by multiple collaborating agents using an Interactive Agents Call Tree architecture, while maintaining long-term memory through knowledge graphs and dynamic variable management [source]","Can observe local files, system processes, web content, and multimodal data (images, audio) with observations distributed across components that can run on different machines. The system maintains conversation context and can dynamically load new observation modules, allowing it to expand its observation capabilities over time [source]","AIlice can execute code, manage files and system processes, analyze data, generate visualizations, interact with web services, process multimodal content (images/audio), and dynamically expand its capabilities by building new interaction modules [source]","Web-based dialogue interface accessible through a browser, allowing users to interact using text or voice commands to issue tasks and receive responses [source]",Unknown,Unknown,Unknown,Available [source],Available [source],Available [source],"Allows for user-interrupt mechanisms, running tasks within secure Docker containers, and warns user to engage in active monitoring [source]",Enables users to manually interrupt its operations through a web-based interface [source],None,None,Demos [source],None,None,None,None,"Integrates with external APIs, Docker containers, inference services like OpenRouter, and supports dynamic module loading through its configuration system  [source]",Github repo has 869 stars and 134 forks [source],
AutoCodeRover,https://web.archive.org/web/20241218004150/https://autocoderover.dev/,"""AutoCodeRover is a technology we are building for enterprises and developers to maintain reliable and performant software systems through autonomous program improvement."" [source]",General-purpose coding and software development,"Unclear, but before 2025. Papers were released April 8, 2024 [source] and August 5, 2024 [source]",https://web.archive.org/web/20241218004150/https://autocoderover.dev/,National University of Singapore,Academic Institution(s),Singapore [source],Unknown,Variable,None,"Model writes a short plan during each step (see figure 1 [source]). Two stages: context retrieval stage (find source of bug) and patch generation stage (fix bug). A ""SpecRover"" version of AutoCodeRover also ""conduct iterative code search accompanied by specification inference"" on the purpose of the code to guide improvements [source].",Uses an abstract syntax tree to represent repositories [source],"Specialized search functions, writing code [source]",Unknown,Unknown,N/A; backends various models,N/A; backends various models,Closed source,Available [source],Closed source,Unknown,Unknown,None,46.2% on SWE-Bench verified [source],Demo [source],None,None,None,None,None,Unknown,
ShowUI,https://github.com/showlab/ShowUI,"ShowUI is a multimodal GUI-based computer use agent that can complete tasks in the web browser, with desktop GUI applications, and on mobile devices [source]","To complete GUI-based computer use tasks across a range of platforms (browser, desktop applications, and mobile) [source]","Github repository created October 31, 2024 [source]",https://web.archive.org/web/20241223004255/https://github.com/showlab/ShowUI,National University of Singapore (et al.) [source],"Academic Institution, Industry Organization",Singapore [source],Unknown,Qwen2-VL-2B with an efficient visual token selection design [source],None,"ShowUI is a single agent system without specialized planning modules, although it does keep a history of previous states and actions in its context. ShowUI is trained to perform next-action predictions and rollouts are autoregressive [source]",A screenshot of the current state as well as the full history of screenshots and actions in the trajectory [source],"For each deployment platform, a readme is provided with the available actions. These typically consist of positional click and type actions as well as device-specific buttons [source]","Users must launch a local server, but then they can issue commands through a webpage and ShowUI will control their MacOS and Windows computers directly [source]","""We utilize 32 V100 GPUs...for two days."" [source]",Open source [source],Open source [source].,Available [source],Basic documentation on Github [source] and pre-print [source],Available [source],None,None,None,"75% ground accuracy on ScreenSpot, 70% success rate on Android in the wild, and 35-37% step success rate on Mind2Web [source]","There is one video demo [source], a HuggingFace space to play with the model [source], and an interface for user-friendly use [source]",None,None,None,None,Can directly control MacOS and Windows computers [source],The Github repository has 39 forks and 684 stars [source],
ChatGPT-OpenAI o1,https://perma.cc/GEC6-ETBX,"A series of frontier large language models ""trained with large-scale reinforcement learning to reason using chain of thought"" [source].","o1 is intended to be used across any domain in which long form reasoning is required. The developers draw particular reference to its ability to be used to construct software development agents, and aid in scientific research [source].","September 12, 2024 [source]",https://web.archive.org/web/20241231025312/https://www.openai.com/,"OpenAI Inc. (parent company). OpenAI Global, LLC (for-profit subsidiary) [source]","The structure of OpenAI is complex. It has a parent 501(c)(3)  non-profit, with a  for-profit subsidiary [source]. OpenAI is restructuring its business into a for-profit benefit corporation that will no longer be controlled by its non-profit board [source]","Incorporation: Delaware, USA (OPENAI GLOBAL, LLC (7208772)) [source]. HQ: California, USA","The OpenAI (non-profit entity) charter states ""We are committed to doing the research required to make AGI safe, and to driving the broad adoption of such research across the AI community"" [source]. They also have a ""Preparedness framework"" for tracking the dangerous capabilities of models they develop and state that ""Only models with a post-mitigation score of ""medium"" or below can be deployed."" [source]","This is a foundation system (including model and scaffolding) in and of itself. It is trained with ""reinforcement learning to perform complex reasoning"" [source]",Public general spec for all OpenAI models [source],Unknown,Textual inputs and images from users [source].,Natural language.,"Users can provide instructions and images via a chat interface or API. When using the ChatGPT interface, the user is presented with a summary of the model's chain of thought, and a final complete answer to their query [source] [source]",Unknown,Closed source,"Closed source. However, the developers report that they use a mixture of public and proprietary data. This data is filtered and refined to ""to maintain data quality and mitigate potential risk."" This includes removing personal information and harmful / sensitive content from training data [source]",Closed source,The API to access the model is documented [source],Closed source,Unknown,Unknown,o1 is available with the ChatGPT Plus and Pro paid tiers [source].,Summary of benchmarking results can be found at [source]. See also MLEBench [source].,Various demos presented [source].,"Safety report [source] with summary [source]. On the OpenAI preparedness scoreboard the model achieved low cybersecurity risk, medium CBRN and persuasion risk, and low model autonomy risk [source]. ","Apollo Research, METR, Faculty, Haize Labs, Gray Swan AI [source].","Teams had access to models via a ""sampling interface or via the API."" [source]","Apollo research tested o1-preview and o1-mini for 'scheming,' broadly defined as ""AIs gaming their oversight mechanisms as a means to achieve a goal."" They found that "" that o1-preview sometimes instrumentally faked alignment during testing."" Based on this red-teaming ""Apollo Research believes that o1-preview has the basic capabilities needed to do simple in-context scheming —scheming which tends to be legible in the model output"" however the ""Apollo team subjectively believes o1-preview cannot engage in scheming that can lead to catastrophic harms, although current evals aren't designed to definitively rule this out."" METR tested o1-preview and o1-mini for ""autonomous capabilities."" The performance of both was not above the best existing public model, Claude 3.5 Sonnet [source]","Other products have started to integrate o1-mini, such as AI enabled coding IDE Cursor [source]. O1 can be can be connected to tools by products such as Devin [source].",Unknown,
OpenAI o3,https://www.youtube.com/watch?v=SKBG1sqdyIU&t=1s,"A series of large language models representing OpenAIs ""next frontier model"" after o1 [source]",O3 is intended to be used across any domain in which long form reasoning is required. The developers draw particular reference to its ability to be used to construct software development agents and solve difficult general reasoning tasks such as competitive math and coding [source].,Not yet externally deployed. First announced on Dec 20th 2024 [source],https://www.openai.com/,"OpenAI Inc.(parent company). OpenAI Global, LLC (for-profit subsidiary) [source]","The structure of OpenAI is complex. It has a parent 501(c)(3)  non-profit, with a  for-profit subsidiary [source]. OpenAI is restructuring its business into a for-profit benefit corporation that will no longer be controlled by its non-profit board [source]","Incorporation: Delaware, USA (OPENAI GLOBAL, LLC (7208772)) [source]. HQ: California, USA","The OpenAI (non-profit entity) charter states ""We are committed to doing the research required to make AGI safe, and to driving the broad adoption of such research across the AI community"" [source]. They also have a ""Preparedness framework"" for tracking the dangerous capabilities of models they develop and state that ""Only models with a post-mitigation score of ""medium"" or below can be deployed."" [source]","Unknown, but if o3 is similar to o1, then the backend model is specifically trained with ""reinforcement learning to perform complex reasoning"" [source]",Public general spec for all OpenAI models [source],Unknown,Textual inputs from users.,Natural language.,"A public user interface not yet released, although most likely will follow previous OpenAI models with a ChatGPT interface and API.",Unknown,Closed source,Closed source,Closed source,N/A (not yet released to public),Closed source,"OpenAI released a new alignment method known as ""Deliberative alignment"" that uses a models reasoning ability to identify and refuse harmful queries. In the resulting paper, they test this method on o3-mini [source]",Unknown,N/A (model not yet released to the public),"Using high test time compute settings, o3 achieved 25.2% on Frontier Math and 87.5% on ARC-AGI semi-private eval set (75.7% on low test time compute). It also has a 2727 Codeforce ELO score and 71.7% on SWE-bench verified. For more benchmarks, and results for the related smaller o3-mini model, see [source].",Release video includes a demo where o3 mini is asked to create code for a website that can be used to submit coding questions to o3 mini and save and run the resulting code. They then use this website to get o3-mini to evaluate itself on GPQA [source].,Some safety evaluations have been run on the o3-mini result. See table 1 of [source].,"No publicly known audits conducted so far, although OpenAI are releasing the model early to safety / security researchers for additional testing [source].",N/A assessments in progress.,Results not yet released.,Model is not publicly released yet.,Unknown,
HyperWrite,https://web.archive.org/web/20241223182033/https://www.hyperwriteai.com/personal-assistant,An AI agent that can control/operate the browser to complete web tasks [source],"A wide array of web-based tasks (e.g. managing emails, placing online orders, LinkedIn searches. etc) [source]","April 12, 2023",https://web.archive.org/web/20241229032251/https://www.hyperwriteai.com/,"OthersideAI, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (OthersideAI, Inc.; 3393499) [source]",Maintains a content policy [source] and terms of use [source],"Can backend various models and has been used with OpenAI models. However, the main system runs custom in-house models. ",None,Users can record their workflows in the browser (ie clicks and key presses) which are then saved and used to instruct the AI agent [source],"The user's browser, including webpages and clicks/key presses; also natural language prompts [source]",Chrome extension that controls the user's browser [source],"The user has access to a 'studio' where they can record workflows, add additional instructions, and run automated workflows [source]",Unknown,Closed source,Closed source,Closed source,Available [source],Closed source,Unknown,Unknown,Usage restrictions are specified in the content policy [source],Unknown,Various demos on the YouTube page [source],None,None,None,None,Chrome extension [source],"Over 100,000 downloads for chrome extension [source]",
SWE-Agent,https://web.archive.org/web/20241127010402/https://github.com/princeton-nlp/SWE-agent,"""SWE-agent lets your language model of choice (e.g. GPT-4o or Claude Sonnet 3.5) autonomously use tools to: fix issues in real GitHub repositories, perform tasks on the web, find cybersecurity vulnerabilities (by solving Capture The Flag challenges), or any custom task."" [source]","General-purpose codin, software development, and web-browsing tasks. ","April 15, 2024 [source]",https://web.archive.org/web/20241127010402/https://github.com/princeton-nlp/SWE-agent,Princeton University [source],Academic Institution(s),"New Jersey, USA [source]",None,Variable. They use GPT 4 turbo and Claude 3 Opus in the paper [source],None,"Has a short ""discussion"" phase before each command [source]. Writes thoughts, actions, observations to its context window. In case of failed actions that get fixed, these are removed. It works using ""configurable agent-computer interfaces (ACIs) to interact with isolated computer environments."" [source]","Inputs, terminal outputs, and filesystem. ","Commands to search, view, and edit files,",The user must run the code manually.,Unknown,N/A; backends various models,N/A; backends various models,Available [source],Available [source],Available [source],None by default except for individual users monitoring and intervening manually. Uses docker containers to ensure reproducible and sandboxed execution [source].,Depends on what is implemented in a specific configuration,None,33.6% on SWE-bench Verified [source],Demo [source],None,None,None,None,"Only coding, terminal, and filesystem interfaces by default. ","1.4k forks, 14.2k stars Available [source]",
Pythagora-v1 (GPT-Pilot),https://web.archive.org/web/20241001171050/https://www.pythagora.ai/v1,"Pythagora-v1 is an application designed to help users interactively design entire apps. ""With Pythagora, people can build apps with up to 5000 lines of code ONLY by writing in natural language."" [source] GPT-Pilot is an open source backend ""brain"" for Pythagora-v1 which is itself agentic [source].",Application development. Pythagora-v1 and GPT-Pilot are marketed as being specialized for making entire apps as opposed to helping with narrower coding tasks.,"GPT-Pilot [source] (the backend ""brain"" for Pythagora-v1) was announced August 23, 2023 [source]. Pythagora-v1 was announced October 1, 2024 [source].",https://www.pythagora.ai/,Pythagora Inc [source],Corporation,"Incorporation: Delaware, USA. HQ: Berkeley, CA, USA [source]",Unknown,Variable including GPT and Claude models [source],None,"There are differently configured specialist agents: ""specification writer"", ""architect"", ""tech lead"", ""developer"", ""code monkey"", ""troubleshooter"", ""debugger"", and ""technical writer"" which each accomplish different parts of the task. Orchestrated by the architect, they iteratively pass documents and instructions between each other. It is able to ask questions to the human overseeing it [source] [source]","By default, GPT-Pilot is limited to observations in the folder that it is working in. It cannot access the web but could be modified to [source]. It is unclear if Pythagora-v1 has the same observation space. ",GPT-Pilot is able to create and work in files and run consequential commands with user approval [source]. It is unclear if Pythagora-v1 has the same action space.,Either through a console or with Pythagora -- a VS Code extension [source],Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Available for GPT-Pilot [source],Available [source],Available for GPT-Pilot [source],"GPT-Pilot is very limited in what it can do directly by default, and is designed to seek user approval before taking certain actions [source]. It is unknown exactly how this design feature in GPT-Pilot affects Pythagora-v1 (which backends GPT-Pilot)",GPT-Pilot is designed to ask users for approval before running certain commands [source]. It is unknown exactly how this design feature in GPT-Pilot affects Pythagora-v1 (which backends GPT-Pilot),"Pythagora-v1 is in a private beta [source]. GPT-Pilot (its backend ""brain"") is open source [source]",None,Demos [source] [source],None,None,None,None,None,GPT-Pilot has 3.2k forks and 32k stars on GitHub [source],
Replit Agent,https://docs.replit.com/replitai/agent,The Replit Agent builds software projects using natural language prompts provided by users.,"Use cases include ""1) Scaffold a new project from scratch; 2) Build and deploy a full-stack application; 3) Automate routine coding tasks; 4) Debug code with AI assistance; 5) Optimize and refactor code; 6) Add features to an existing app; 7) Learn while you build; 8) Generate documentation; 9) Create a database and connect it to your app; 10) Deploy applications to the cloud"" [source]",September 2024 [source],https://docs.replit.com/category/replit-ai,"Replit, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (Replit, Inc (6008239)) [source]",Unknown,Unknown,None,Unknown,Common observation space is user input and outcome of agent actions.,Unknown,"""Replit Agent is a coding assistant built right into the Replit IDE"" [source]",Unknown,Unknown,Unknown,"Replit has source code for most of the products it offers [source], along with a repository for available LLMs [source]. However there is no source code about the Replit Agent and whether their own models were used to create it.",Available [source],Closed source,"They have staff who can report content that violates Replit's Terms, Community Standards, or Privacy Policy [source].",Serious violations may lead to users getting banned from Replit [source].,None,Unknown,Unknown,None,None,None,None,"None highlighted in particular for the Replit Agent. However, Replit has many integrations available [source]. ","""The Replit Agent is currently an experimental product"" and ""currently available through an early access program"" [source]",
The AI Scientist,https://web.archive.org/web/20241221112103/https://github.com/SakanaAI/AI-Scientist?tab=readme-ov-file,An open-source AI agent designed to automate scientific research [source].,"Conducting end-to-end scientific research projects, including idea creation, to experimental design, experimental execution, result analysis, and paper writing [source].","August 16, 2024",https://web.archive.org/web/20241231143155/https://sakana.ai/,"Unknown (seemingly ""Sakana AI"")",Unknown,Japan [source],Unknown,"Variable, including GPT-4o, GPT-4o-mini, and o1 models, LLaMA 3, and Claude Sonnet 3.5 [source]",None,"The agent implements research in 4 stages: (1) Idea Generation. (2) Experiment iteration. (3) Paper writing. (4) Paper review. To begin, the agent is provided a code template that implements some baseline experiment, a LaTeX template, and plotting file template. Each stage of the above research pipeline is completed by prompting the agent and providing it with text state summarizing previous sections [source].",The AI Scientist operates in an environment where it can see the output of executed code and has access to an archive of ideas.,Writing and executing code and recording ideas in a scratchpad.,The user simply runs scripts that produce and save output files.,"Unknown  However, they report a cost of about $15 per paper. ",N/A; can use various backend models,N/A; can use various backend models,Open source [source],Available [source],Available [source],Developers recommend running the model in a sandboxed virtual environment.,Depends in what environment the agent is run in.,None,Unknown,Authors evaluate the quality of papers written by the AI Scientist using the AI scientist reviewer and manual inspection.,"The technical report contains a ""Limitations and Ethical Considerations"" that discusses the safety implications. The authors state, ""it could be explicitly be deployed to conduct unethical research, or even lead to unintended harm if The AI Scientist conducts unsafe research. Concretely, if it were encouraged to find novel, interesting biological materials and given access to 'cloud labs' (Arnold, 2022) where robots perform wet lab biology experiments, it could (without its overseer’s intent) create new, dangerous viruses or poisons that harm people before we can intervene."" ",None,None,None,"The AI Scientist uses Aider [source], an LLM-Based Coding Assistant, to write experimental code.","The github repository for the AI Scientist has 1.2k forks, and 8.4k stars [source].",
Agentforce Agents,https://web.archive.org/web/20241229094143/https://www.salesforce.com/agentforce/?bc=HA,"Agentforce is a service where Salesforce customers can deploy pre-built, customizable AI agents to automate simple business tasks [source]","Applications include shipping management, sales coaching, technical support, appointment management, content creation, and sales development [source]","Made available on October 14, 2024 [source]",https://web.archive.org/web/20241231231522/https://www.salesforce.com/uk/?bc=WA,"Salesforce, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (SALESFORCE INC (multiple file numbers)) [source]. HQ: San Francisco, CA, USA [source]",Artificial Intelligence Acceptable Use Policy [source] and Salesforce’s Trusted AI Principles [source].,"Agentforce is built on Salesforce's proprietary AI models, including xGen-Sales, xLAM and the Atlas Reasoning Engine [source] [source] [source]",Unknown,"Planning and Reasoning: Agentforce uses ReAct prompting, where ""the system goes through a loop of reason, act, and observe until a user goal is fulfilled."" Memory: The Salesforce data cloud unifies ""all customer data and metadata across systems in real time, enabling Agentforce to operate with complete context and precision."" [source] [source]","Agentforce agents operate in the Salesforce ecosystem, inducing Flow, Customer 360, Apex, Datacloud, and MuleSoft (which acts as a ""unified solution for integration and APIs"") [source] [source] [source]","Agentforce agents take actions in the Salesforce ecosystem, inducing Flow, Customer 360, Apex, Datacloud, and MuleSoft (which acts as a ""unified solution for integration and APIs"").  [source] [source] [source]","""Agent Builder is the low-code builder for Agentforce [...] Setup is simple: Create a job to be done by the Agent by defining topics, giving natural language instructions for that topic, and creating a library of actions for it to choose from. Easily monitor an Agent’s plan of action and test its responses right in Agent Builder."" [source]",Unknown,"While Agentforce is closed source, a 1B quantized version of their backend model is available: ""xLAM-1B, specifically, is a non-commercial, open-source model to help advance the science with the research community, while Salesforce uses a much more performant model for Agentforce."" [source] [source]",A small subset of the training data is available on a CC-BY-4.0 license [source] [source],"While Agentforce is closed source, a 1B quantized version of their backend model is available: ""xLAM-1B, specifically, is a non-commercial, open-source model to help advance the science with the research community, while Salesforce uses a much more performant model for Agentforce."" [source] [source]",Available [source],Closed source,"""With Agentforce, teams can use natural language topics and instructions to create guardrails for an agent, including when to escalate or hand off a task to a human. The Einstein Trust Layer enables Agentforce to use any LLM safely by ensuring that no Salesforce data is viewed or retained by 3rd-party model providers."" [source]",Unknown,None,"Agentforce has not been publicly tested on canonical benchmarks like AgentBench, SWE-Bench, GAIA, and MLE-Bench. However, their backend model temporarily secured first place on the Berkeley Function-Calling Leaderboard [source] ",Demo [source],None,Internal (Salesforce) and external (unknown).,"Salesforce subjected their ""AI agents to over 8,000 adversarial inputs to pressure-test their boundaries."" They also outsourced testing to external vendors, ""which simulated attacks on our product using adversarial prompts with the intention of making the product generate biased or toxic outputs."" [source] [source]",Limited information about their findings is available at [source].,"Agentforce agents operate in the Salesforce ecosystem, inducing Flow, Customer 360, Apex, Datacloud, and MuleSoft (which acts as a ""unified solution for integration and APIs""). Additionally, by default, Agentforce can connect to a variety of messaging platforms like WhatsApp and Messenger [source] [source] [source]","Salesforce reports bringing ""5,200 customers live on Agentforce in their sandboxes in the first two days."" [source]",
Agent S,https://arxiv.org/abs/2410.08164v1,"""A novel framework for developing fully Autonomous Graphical User Interface (GUI) agents that can perform a wide range of user queries by directly controlling the keyboard and mouse."" [source]","Automating complex, multi-step tasks by getting an agent to use computers like a human [source]","Earliest GitHub commits from October 11, 2024 [source]",https://www.simular.ai/,"Simular, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (SIMULAR, INC (7608865)) [source]",Terms of Use [source],"Variable, defaulting to GPT-4o and Claude-3 Sonnet [source]",None,"Agent S leverages ""an experience-augmented hierarchical planning method that uses experience from external web knowledge and the agent’s internal memory to decompose complex tasks into executable subtasks."" The authors illustrate their implementation through Figures 3 and 4 in [source].","Agent S operates in an environment where it can observe screenshots of webpages and annotated GUI elements, along with its task memory [source]","The agent's design ""incorporates a bounded action space. This space includes primitive actions like click, type, and hotkey."" [source]","While the company's demos sometimes include a user interface (UI), there is no publicly available UI [source] ",Unknown,N/A; backends various models,N/A; backends various models,Available [source],Documentation on GitHub [source] and pre-print [source],Available [source],"The authors bound the agent's action space, in part, to improve its safety [source]",Depends on what is implemented in a specific configuration [source].,None,20.58% success rate on the OSWorld full test set when running on GPT-4o [source],Demo [source],None,None,None,None,Agent S was designed for the Ubuntu operating system generalizes to the Windows operating system [source],The GitHub repository has 99 forks and 734 stars [source],
OS-Copilot,https://os-copilot.github.io/,"OS-Copilot is ""a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications"". FRIDAY is ""a self-improving embodied agent for automating general computer tasks"", built using the OS-Copilot framework [source]","Potential use cases include operations requiring interactions with BASH, a Python interpreter, the keyboard/mouse, and an API. More details can be found in Appendix C [source]","Paper arXived February 12, 2024 [source]",https://os-copilot.github.io/,Shanghai AI Laboratory (et al.) [source],Unknown,China [source],Unknown,"Variable for OS-Copilot. For FRIDAY, some backend models they have used include GPT-4 and GPT-4-Turbo plugins.",None,"Implementation of OS-Copilot consists of three components: 1) A planner that ""reasons over user requests and decomposes complex ones into simpler subtasks""; 2) A configurator, with working memory, that ""takes a subtask from the planner and configures it to help the actor complete the subtask""; and 3) An actor that consists of an executor, proposing and executing appropriate actions, and a critic, assessing the outcomes of the executor's actions [source] ",Common observation space is user input and outcome of agent actions.,"The OS-Copilot framework ""consolidates common practices for OS manipulation, including Python code interpreter (Significant-Gravitas, 2023), bash terminal, mouse/keyboard control (Cheng et al., 2024), and API calls (Qin et al., 2023)"" [source]",There is a user interface to interact with OS-Copilot [source].,Unknown,N/A; backends various models,N/A; backends various models,Available [source],Available [source],Available [source],Unknown,Unknown,None,"On ""GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%"" (40.86% on level 1, 20.13% on level 2,  6.12% on level 3 tasks). Additionally, on the SheetCopilot-20 dataset, FRIDAY passes 60% of the tasks with each task being performed only once [source]",FRIDAY is essentially a demo for OS-Copilot [source].,None,None,None,None,"FRIDAY can work with other systems since it is equipped with tools for OS manipulation that include interacting with a Python code interpreter, bash terminal, mouse/keyboard control, and API calls.",The OS-Copilot github repository has 177 forks and 1.6k stars [source].,Some of the same developers have released a foundation model known as OS-ATLAS to serve as a backend for agents [source]
Simple AI,https://web.archive.org/web/20241210031952/https://usesimple.ai/,Simple AI can make AI powered phone calls on your behalf [source],"Simple can wait on hold and navigate phone trees. Simple can make phone calls in 33 countries using 15 different languages, or even call 10 different stores at the same time [source]","December, 5th 2024 [source]",https://web.archive.org/web/20241210031952/https://usesimple.ai/,Simple AI Lab Company [source],Corporation,"Incorporation: Delaware, USA (SIMPLE AI LAB COMPANY 3588956) [source]",Unknown,Unknown,None,Unknown,Unknown,"The system is able to make calls, navigate phone trees, make reservations/place orders [source]",The user interacts with Simple AI through a chat interface [source],Unknown,Unknown,Unknown,Unknown,Unknown,Unknown,None,None,None,None,None,None,None,None,None,NA,None,
OpenVLA,https://arxiv.org/abs/2406.09246,"OpenVLA is a 7 billion parameter open-source Vision Language-Action (VLA) model. This transformer based model accepts textual descriptions of tasks, and visual inputs, and outputs actions that can be executed by a robot. OpenVLA can be used to control many different types of manipulation robots, making it a generalist robot policy.",Robotic control.,"First paper release June 13, 2024 [source]",https://web.archive.org/web/20241222012031/https://openvla.github.io,Stanford University (et al.) [source],Academic Institution(s),"California, USA [source]",None,"Llama-2 7b vision-backbone, SigLIP and DinoV2 vision encoders.",None,Maps language task description and current visualstate directly to robot actions. No explicit planning beyond what is learned internally from the training data.,"Textual inputs describing the robot manipulation task, and 224 × 224 pixel images of the current world state the robot is acting in.","7-dimensional robot control action (e.g. spatial dimensional, end-effector orientation, and grip strength) represented as discrete tokens.",N/A; an engineering project,"""The final OpenVLA model is trained on a cluster of 64 A100 GPUs for 14 days, or a total of 21,500 A100-hours, using a batch size of 2048."" [source]",Open source [source].,"OpenVLA starts using a pretrained language model backbone and vision encoders. OpenVLA then finetunes on a curated subset of the Open X-Embodiment robotics dataset (that consists ""more than 70 individual robot datasets, with more than 2M robot trajectories"" [source])",Available [source].,"Unavailable, but they have a technical report [source]",Available [source].,None,"The model has no shutdown procedures, however it is a base model.",None,"On the BridgeData V2 and Google robot evaluate OpenVLA outperforms the previous SoTA open source manipulation policy, Octo (93M parameters) and SoTA closed source manipulation policy, RT-2-X (55B parameters).","The authors test the ability of OpenVLA to be adapted to new settings using small data sets of 10-150 demonstrations of some target task. They find that generalist policies like OpenVLA and Octo perform better in target tasks that ""that involve multiple objects in the scene and require language conditioning,"" but Diffusion Policy imitation learning techniques work better in ""narrower single-instruction tasks"" [source].",None,None,None,None,"OpenVLA can be used to control robots within its training dataset, and finetuned for new systems.","The github repository for the OpenVLA bas 173 forks, and 1.4k stars [source].",
Virtual Lab,https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1,"The Virtual Lab consists of an LLM principal investigator agent guiding a team of LLM agents with different scientific backgrounds (e.g., a chemist agent, a computer scientist agent, a critic agent), with a human researcher providing high-level feedback [source]","The Virtual Lab is intended to facilitate interdisciplinary scientific research by combining AI agents and human collaboration. It uses LLMs to simulate a research team with a principal investigator, scientist agents, and a critic agent to address complex, open-ended problems. Its capabilities were demonstrated in designing nanobody binders for SARS-CoV-2 variants, integrating tools like AlphaFold and Rosetta. The goal is to enable impactful, real-world discoveries by bridging expertise across fields​ [source]",November 26th 2024 [source],https://web.archive.org/web/20241126222051/https://github.com/zou-group/virtual-lab,Stanford University (et al.) [source],Academic Institution(s),"California, USA [source]",None,"It's powered by large language models (LLMs), specifically GPT-4o, which drives the reasoning abilities of its agents. It also incorporates computational tools such as ESM (a protein language model), AlphaFold-Multimer (a protein folding model), and Rosetta (a computational biology software) for specific tasks within its nanobody design pipeline [source]",None,"it uses a multi-agent architecture to simulate reasoning, planning, and memory through structured interactions. A PI agent leads the process, setting research directions and summarizing team discussions, while specialist agents contribute expertise from their defined scientific domains. Reasoning is achieved via iterative team and individual meetings, where agents collaborate, critique each other’s inputs, and refine their outputs across multiple rounds. Memory is implemented through agent-written summaries of previous meetings, which inform ongoing decisions, ensuring continuity and context throughout the research project​ [source]","The observation space includes inputs provided by the human researcher, such as agendas, scientific contexts, rules, and relevant datasets. During meetings, agents can observe each other's contributions, critiques, and outputs, enabling collaborative reasoning. For specific tasks, agents can access computational tools like ESM, AlphaFold-Multimer, and Rosetta to analyze data and generate results. The system also uses memory elements, such as summaries of prior meetings, to maintain awareness of past discussions and decisions​ [source]","The Virtual Lab is designed to handle a wide range of tasks through its AI agents. It can come up with and refine scientific ideas during team and one-on-one meetings, promoting collaboration across different fields. The system can also design workflows for research projects (designing nanobodies being the one exposed in the main article), using tools like ESM, AlphaFold-Multimer, and Rosetta. On top of that, it can write and run code to analyze data and fine-tune results, with feedback from the Scientific Critic agent to make sure everything stays up to standards [source]","Users interact with the Virtual Lab primarily through an interface that allows them to provide high-level guidance. This includes defining the research agenda, setting goals, and specifying rules for the agents. The system also allows users to write summaries and provide feedback. The interactions are structured around meetings where users can input specific agendas and questions, which guide the agents in their collaborative problem-solving process​ [source]",Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Available [source],Available [source],Available [source],"The system employs iterative refinement, where outputs are improved over multiple rounds of discussion, with their Scientific Critic Agent providing feedback. Additionally, predefined rules and constraints guide the system's actions, preventing harmful decisions [source]","Not formally defined, but there are detailed mechanisms of controls and guardrails that could potentially shut down the system if needed [source]",None,Unknown,The paper presenting Virtual Lab demonstrates designing nanobody binders to recent variants of SARS-CoV-2 [source],None,None,None,None,"ChatGPT (GPT-4o), ESM, AlphaFold-Multimer, and Rosetta to support tasks like protein design, structure prediction, and AI-driven scientific research [source]",Unknown,
SuperCoder 2.0,https://web.archive.org/web/20241228215004/https://superagi.com/supercoder/,Open source autonomous software development system [source],General-purpose coding and software development,"Earliest GitHub commits from July 3, 2024 [source]",https://web.archive.org/web/20241228215051/https://superagi.com/,"SuperAGI, Inc [source]",Corporation [source],USA [source],Unknown,"Variable, including Claude 3.5 Sonnet and GPT-4o",None,"It uses its ""PlannerAgent, a sophisticated tool designed to navigate through the code files, identify buggy locations, and determine the necessary changes required to rectify the issues [...] In some instances, the PlannerAgent may need to insert an entirely new method to resolve the problem or at least propose a viable solution"". SuperCoder 2.0 uses retrieval augmented generation and its 'AssimilatorAgent' to retrieve relevant files [source].","SuperCoder 2.0 looks at an entire codebase, using retrieval augmented generation and an agent to select specific files [source].","SuperCoder 2.0 looks can retrieve, generate, and replace code in a codebase [source].",They have a visual application user interface where a user can specify tasks for the agent [source].,Unknown,N/A; backends various models,N/A; backends various models,Available [source],Available [source],Available [source],None,Unknown,None,"""SuperCoder 2.0 achieves 34% success rate in SWE-bench Lite, ranking #4 globally and #1 among all open-source coding systems"" [source].",None,None,None,None,None,"Flask, Django, NextJS [source]",Has 863 stars and 83 forks on GitHub [source],
data-to-paper,https://web.archive.org/web/20241119044950/https://github.com/Technion-Kishony-lab/data-to-paper?tab=readme-ov-file,"A research automation platform completing a stepwise research process, able to design research plans, raise hypotheses, write and debug analysis code, and create complete and information-traceable papers [source].","Research automation/copiloting and acceleration, while supporting transparency and traceability of decisions in the automated research process. Supports autonomous goal-search and fixed-goal specification.","ArXived April 24, 2024 [source]",https://web.archive.org/web/20241221170344/https://kishony.technion.ac.il/,Israel Institute of Technology (et al.) [source],"Academic Institution, Industry Organization",Israel [source],None,"Multiple different LLMs used / experimented with, found that open-source models (Llama2 family+ CodeLlama-34b) hallucinated far more than OpenAI models (GPT-3.5-turbo, GPT-4).",None,"The system walks through the data research process step-by-step, through a series of predefined research steps and tool use calls. Steps are occasionally vetted with external LLM review, and the calls are broken up sufficiently to increase modularity (and hence reliability) of the individual steps. ","Depending on the stage of the research generation process, can observe facts about data, generated research goal, analysis code, and sections of the generated report. In general, data-to-paper observes the union of the data and provided descriptions, and a subset of previously generated output. See Figure 1B [source]","Writing and executing analysis code, exploring data and metadata, searching the literature through Semantic Scholar API, and writing and compiling hyperlinked LaTex papers section-by-section.","Can provide data, data description, and optionally a fixed analysis goal for the system to produce. There is also a GUI app that allows users to use the system in a copilot mode [source] ",Unknown,N/A; backends external model(s) via API,N/A; backends external model(s) via API,Available [source],Available [source],Available [source],"System is sufficiently scoped and modular that harm seems implausible, although no specific measures are taken to ensure safety.",None,None,None,Demo paper generated available at [source],None,None,None,None,The semantic Scholar API and is interchangeable with other literature search engine APIs,GitHub repo has 50 forks and 489 stars [source],
SeePlanAct,https://web.archive.org/web/20240906205347/https://assistantbench.github.io/,"""A web agent equipped with memory and planning components for multihop, info-seeking questions."" [source]","SPA was built to tackle tasks in AssistantBench: a benchmark that ""evaluates the ability of web agents to automatically solve realistic and time-consuming tasks."" [source]","Earliest GitHub commits from July 13, 2024 [source]",https://web.archive.org/web/20240906205347/https://assistantbench.github.io/,Tel Aviv University (et al.) [source],Academic Institution(s),Israel [source],"None  but see the ""Ethical Implications and Broader Impact"" section of the paper [source]","Variable, since SPA is built on SeeAct, which is compatible with several backend models. The developers mainly use GPT-4T and Claude-3.5-Sonnet [source] [source]",None,"SPA has ""two specialized components: (1) a planning component for the model to plan and re-plan its execution, and (2) a memory component with the option to transfer information between steps via a memory buffer."" The authors provide the prompt that they use to achieve this in Figure 20 of [source].","SPA operates in an environment where it can observe webpage screenshots and HTML elements, along with its task memory [source]","SPA can take the following actions when browsing the internet: ""Click, Select, Type, GoTo, Search, GoBack, Scroll, Press Enter, Terminate."" [source]",Code released in a GitHub repository without a user interface [source],Unknown,N/A; backends various models,N/A; backends various models,Available [source],Documentation on GitHub [source] and pre-print [source],Available [source],"The system is prompted to ""not attempt to create accounts, log in or do the final submission"". Individual users can also monitor and intervene manually [source]","The model is prompted to terminate a task ""if it requires potentially harmful actions."" [source]",None,SPA is evaluated on AssistantBench and FanoutQA [source],SPA achieves 12.9% on a benchmark that was constructed by the authors to test for autonomous task execution (AssistantBench) when leveraging Claude 3.5 Sonnet [source],None,None,None,None,"By default, SPA can navigate the internet and interact with web tools like Google Maps. Beyond this, interoperability is not highlighted in particular [source]",The GitHub repository has 2 forks and 41 stars [source],
Trase Agent,https://web.archive.org/web/20241203022621/https://www.trasesystems.com/,Trase agents are advertised as being able to automate a variety of tasks [source].,Primarily to automate tasks in the healthcare and energy fields [source],Unknown,https://web.archive.org/web/20241203022621/https://www.trasesystems.com/,"Trase Systems, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (TRASE SYSTEMS, INC (7608371)) [source]",Unknown,"Variable, including models OpenAI and Gemini models [source]. ",None,Unknown,Unknown,Unknown,Unknown,Unknown,Closed source,Closed source,Closed source,Unavailable,Closed source,Unknown,Unknown,None,39.53 on GAIA benchmark [source],None,None,None,None,None,Unknown,Unknown,
AutoWebGLM,https://arxiv.org/abs/2404.03648,AutoWebGLM is an automated web navigation agent [source],Web browsing tasks [source],"Paper arxived on April 4, 2024, and Github code initial commit April 3, 2024 [source] [source]",https://github.com/THUDM/AutoWebGLM,Tsinghua University (et al.) [source],"Academic Institution, Corporation",China [source],None,ChatGLM3-6B model [source],None,"Paper arXived on April 4, 2024, and Github code initial commit April 3, 2024 [source] [source]","The observation space consists of states that include simplified HTML information, current location within webpage, and past operation records [source].","The action space includes the following: Click at an element, Hover on an element, Select option in an element, Type to an element, Scroll up or down of the page, Go forward or backward of the page, Jump to URL, Switch to i-th tab, Notify user to interact, Stop with answer [source]",A Chrome extension where users write prompts to perform operations on websites [source],Unknown,Available [source],Available [source],Available [source],"Some documentation in Github [source], and arXiv [source]",Available [source],Unknown,Unknown,None,"Mind2Web (59.5), MiniWoB++ (89.3%), WebArena (18.2%) [source]",Reported results on their own benchmark called AutoWebBench [source],"The authors ""identify errors that occasionally occur during task execution, which can be broadly categorized into four types: hallucinations, poor graphical recognition, misinterpretation of task context, and pop-up interruptions"". In Table 7, they report how often these types of errors occur [source]",None,None,None,Chrome [source],65 forks and 784 stars on GitHub [source],
WebRL,https://arxiv.org/abs/2411.02337v1,"WebRL is a ""reinforcement learning framework designed to train high-performance web agents using open LLMs.""  [source]",Intended to be used to train agents that can accomplish tasks (described in natural language) on the internet through a web browser [source],Paper first put on arXiv on November 4th 2024 [source],https://web.archive.org/web/20241107151506/https://github.com/THUDM/WebRL,Tsinghua University (et al.) [source],Academic Institution and corporation.,China [source],None,"The authors release three agents trained using WebRL based on Llama-3-8b, Llama-3-70b, and GLM-4-9b [source]",None,Reinforcement learning is used to train the model to plan internally. That is there are no separate specialized planning modules [source],HTML content of the current web page along with the history of previous actions [source],"In principle, WebRL can be used to train agents using any natural languages based action space. The authors use WebArena to train their released models, an environment with an ""action space that emulates the keyboard and mouse operations available on web page"" [source].","There is no publicly available UI. Users can download the models released by the authors and personally host them. The agents are designed to be interacted with using natural language descriptions of tasks, just like a regular chatbot.",Unknown,Authors release three open source agents created using the WebRL framework [source].,Open source [source].,Open source [source],Basic documentation on Github [source] and paper [source],Public (see open source code).,None,None,None,Agent based on Llama-3-70b achieves 49.1% success rate on WebArena Lite [source].,Paper contains various ablation studies [source].,None,None,None,None,Integration with WebArena [source].,Paper repository has 113 stars and 4 forks [source].,
Proposer-Agent-Evaluator,https://web.archive.org/web/20250115041519/https://yanqval.github.io/PAE/,A system for vision foundation models to propose and practice skills in new environments [source],Complete general web-based tasks,"December 17, 2024 [source]",https://web.archive.org/web/20250115041519/https://yanqval.github.io/PAE/,University of California Berkeley (et al.) [source],"Academic Institutions, Industry Organization","California, USA [source]",None,LLaVa-34B and -7B [source],None,PAE uses chain-of-thought reasoning before taking actions [source].,PAE's observation space consists of a screenshot of the current web page with the different interactive elements identified [source].,"""The action space contains primitive web operations such as clicking on links and typing into text boxes"" [source].",The primary way to interact with PAE is with their open-sourced code and API [source].,"30k trajectories * 1/512 hour/trajectory = ~60 hours of AWS P4 instance (8xA100 40G) per task, see Figure 6 [source]",Available [source],Available [source],Available [source],Available [source],Available [source],"PAE allows a maximum of 10 steps. Additionally, it has a system prompts which steers the system away from harmful/unhelpful behaviors [source].",N/A Open-source,None,"WebVoyager (33%), WebArena Easy (25.7%)  [source]",They test generalization on 85 real-world websites not included in other benchmarks. They test the automated evaluator with human baselines [source].,None,None,None,None,Any web interface,Github repo has 42 stars and 2 forks [source].,
Octo,https://arxiv.org/abs/2405.12213,"Octo is an open-source generalist robot policy designed for robotic manipulation tasks. There are two released Octo-models, Octo-small and Octo-base, both transformer models with 27M and 93M parameters respectively.",Robotic control.,"First paper released on May 20, 2024 [source]",https://web.archive.org/web/20250105113842/https://octo-models.github.io/,University of California Berkeley (et al.) [source],Academic Institution(s),"California, USA [source]",None,"The main Octo model is trained from scratch. However, language inputs are first processed by an 11M parameter t5-base model, and the resulting embeddings are processed by the Octo model.",None,"Octo maps natural language or image depiction of goal states, and image representations of the current state. to robot actions. No explicit planning is used beyond what is learnt internally from the training data.","Textual or image inputs describing goal states, and image inputs describing the current world state.",The action space is flexible. The model outputs action embeddings that are converted to specific actions by task specific action heads (that are diffusion based).,N/A; an engineering project,"Octo-base ""was trained for 300k steps with a batch size of 2048 using a TPU v4-128 pod, which took 14 hours. A finetuning run of the same model on a single NVIDIA A5000 GPU with 24GB of VRAM takes approximately 5 hours and can be sped up with multi-GPU training.""",Open source [source].,Octo is trained on a curated subset of the Open X-Embodiment dataset.,Available [source].,"Unavailable, but they have a technical report [source].",Available [source].,None,"The model has no shutdown procedures, however it is a base model.",None,"The authors evaluate Octo's ability across 9 robot learning tasks, testing both 0-shot and task specific finetuning performance. The authors find performance comparable to or exceeding RT-1-X and RT-2-X [source]","The authors evaluate Octo's ability across 9 robot learning tasks, testing both 0-shot and task specific finetuning performance. The authors find performance comparable to or exceeding RT-1-X and RT-2-X [source]",None,None,None,None,"Octo can be finetuned to control different manipulation robots, specifically by finetuning new action heads.","The github repository for the Octoi bas 160 forks, and 839 stars [source].",
Aguvis,https://arxiv.org/abs/2412.04454,"Aguvis is fully autonomous pure vision GUI agent capable of performing tasks independently without relying on proprietary models and it can operate across various platforms (web, desktop, mobile) [source]",GUI automation - Autonomously navigate and interact with complex digital environments [source],"No apparent official deployment or release but earliest github commits are at Dec 23, 2024 [source]",https://aguvis-project.github.io/,University of Hong Kond (et al.) [source],"Academic Institution, Corporation","Hong Kong, China [source]",None,"Uses Qwen2-VL as the backend Vision-Language Model. Aguvis also evaluates LLaVA-OneVision as an alternative backend, demonstrating that its performance is independent of the VLM used. Aguvis can operate autonomously or serve as a grounding model when paired with GPT-4o for planning [source] ",None,"An Aguvis agent receives an image observation from the GUI environment, reasons and generates an inner monologue based on its previous actions and observations. This inner monologue consists of three components: a natural language description of the current observation, internal reasoning based on the high-level goal, the observation description, and previous thoughts and a low-level action instruction in natural language that specifies the next action. The agent then executes the action based on the instruction, receives a new observation, and repeats this process until it either achieves the goal or reaches a terminal state. Aguvis is trained on collection of GUI interaction trajectories called the AguVis collection (Appendix B.1 in [source]). The training is done in two stages: first stage enables the model to understand and interact with objects within a single GUI screenshot. The second stage introduces more complex decision-making and reasoning processes. This phase is designed to teach the model how to execute multi-step tasks by reasoning through agent trajectories that vary in complexity and environments, encompassing diverse reasoning modes [source]",Current image observation from the GUI environment. The system also has access to previous observations and actions.  [source],pyautogui [source] support for both basic and pluggable actions systems [source],Users provide task prompts to the system and can observe the system's reasoning process and outputs [source],AGUVIS is trained on a cluster of H100-80G GPUs: AGUVIS-7B uses 8 nodes and completes the grounding training within 5 hours and planning and reasoning training within 1 hour. AGUVIS-72B uses 16 nodes and completes the grounding training within 30 hours and planning and reasoning training within 6 hours [source],Available [source],Aguvis collection splits used for both training stages are available [source],Available [source],Preliminary [source],Available [source],Unknown,Unknown,None,OSWorld (14.79% average for 7B modelwithGPT-4o planner and 10.26% average for 72B model) [source],Available [source],None,None,None,None,Aguvis provides cross-platform operability and supports both offline and real-world online scenarios [source].,11 forks 170 stars on GitHub [source],
CodeActAgent,https://arxiv.org/abs/2402.01030,CodeActAgent can autonomously execute code and self-debug to carry out programming tasks [source],"""CodeActAgent, designed for seamless integration with Python, can carry out sophisticated tasks (e.g., model training, data visualization) using existing Python packages."" [source]","February 1, 2024 [source]",https://web.archive.org/web/20241223175319/https://github.com/xingyaoww/code-act,University of Illinois Urbana-Champaign (et al.) [source],"Academic Institution, Industry Organization [source]",USA [source],"None  but see the ""Impact Statement"" in the paper [source]","Variable, defaulting to Llama2 and Mistral [source]",None,"CodeActAgent plans for its action through chain-of-thought. It also uses automated feedback from programming terminals (e.g., error messages) to self-debug its code [source]","""For each turn of interaction, the agent receives an observation (input) either from the user (e.g., natural language instruction) or the environment (e.g., code execution result)."" [source]","CodeActAgent can execute code actions in a Python terminal, which may then call an available API [source]","While the company's demos sometimes include a user interface (UI), there is no functioning, publicly available UI.","""All SFT [supervised fine-tuning] experiments are performed on one 4xA100 40GB SXM node using a fork of Megatron-LLM with a training throughput of around 9k tokens per second."" [source]",Backends external models. Weights are available [source],Backends external models. Fine-tuning data is open-sourced [source],Available [source],Basic documentation on Github [source] and pre-print [source],Available [source],None,None,None,46.2% on Miniwob++ when the backend model is Mistral 7B [source].,CodeActAgent performs well on a benchmark that was constructed by the authors to test for tool composition (M3ToolEval) [source]. A demo is on the GitHub repo [source].,None,None,None,None,"By default, CodeActAgent is integrated with a Python interpreter and can leverage existing Python packages. By providing API calls as Python functions, CodeActAgent can search Wikipedia and control robots. As CodeActAgent is open-source, it can be modified to integrate with other systems [source]",GitHub repo has 519 stars and 40 forks [source],
DynaSaur,https://arxiv.org/abs/2411.01747,An LLM agent which can dynamically synthesize its own actions by writing Python code.,Designed for domains where the number of possible actions is impractical to completely exhaust. Particularly applicable in strongly open-ended domains. General-purpose tool.,"Paper ArXiVed Nov 4 2024, initial commit to GitHub repo Nov 8, 2024 [source]",https://web.archive.org/web/20241206200508/https://github.com/adobe-research/dynasaur,University of Maryland (et al.) [source],"Academic Institution, Corporation",USA [source],Unknown,LLM backend uses GPT-4o and GPT-4o mini,None,"The agent is initially pre-populated with initial actions and a task description: observing these, the agent can interact with an iPython kernel, and generate new actions for itself to execute, in the process of completing the task. ","The agent observes the set of predefined actions, the task description (provided by the end users), and its current trajectory (represented as a chain of thought-action-observation).  ","The agent proposes actions represented in Python functions, which can be but is not limited to the user-defined actions or compositions thereof. These Python functions can in turn interact with the internet, the OS, or anything else. Notably, the agent can call an action retriever, which retrieves Python descriptions of actions previously generated (not included by default due to context window concerns), which is then returned in the subsequent step as part of the observation.","The user provides an initial task description, but otherwise doesn't interact with the system once deployed.",Unknown,Available [source],Open sourced on GitHub [source],Open sourced on GitHub [source],Open sourced on GitHub [source],Open sourced on GitHub [source],Unknown,Unknown,None,"GAIA 7th leaderboard rank [source] (first at time of publication), 27% for GPT-4o-mini-powered agent and 38% for GPT-4o-powered agent. ",GAIA benchmark [source],None,None,None,None,Can work in principle with anything that interacts with Python.,18 forks and 228 stars on GitHub [source],
Weco,https://web.archive.org/web/20241127010507/https://www.weco.ai/,An AI data science agent: AIDE designs pipelines for data analysis by generating code and producing models to analyze data [source],"Weco ""generates code for data preprocessing as well as model training, inference, and evaluation...The current alpha version of AIDE primarily targets tabular data tasks that can be solved with CPUs."" [source]","April 4, 2024 [source]",https://web.archive.org/web/20241127010507/https://www.weco.ai/,WECO AI LTD [source] [source],Private limited Company (UK) [source],Incorporation: UK [source]. HQ: London [source],Unknown,Variable including OpenAI or Anthropic models [source],None,"""Solution Space Tree Search:"" (1) Proposes solutions or makes changes to existing ones, (2) evaluates quality of solutions by running them and evaluating results, (3) selects most promising solution and begins another round of iteration/refinement [source]. Uses a 'journal' structure which stores generated code samples, tree structure of generated code samples, results of code execution, and evaluation metrics [source].",Maintains a workspace with all of the files and data generated by the AI agent [source],"Writes and executes code, python interpreter, directory for storing logs [source]",The user can monitor the agent's logs and the forming solution tree [source],Unknown,N/A; backends various models,N/A; backends various models,Available [source],Available [source],Open source [source],Depends on what guardrails are implemented in a specific configuration,Depends on what is implemented in a specific configuration,None,"On MLE-Bench, ""OpenAI's o1-preview with AIDE scaffolding — achieves at least the level of a Kaggle bronze medal in 16.9% of competitions"" [source], which was the best reported score; OpenAI used Weco AI's open source scaffolding for their benchmarking",Several sample results Available [source],None,None,None,None,None,Unknown,
XBOW,https://web.archive.org/web/20241231073112/https://xbow.com/,XBOW autonomously identifies vulnerabilities/exploits in web settings and produces patches [source],Improving offensive security on the web [source],"Announced in a blog post on July 15, 2024 [source], but has not yet publicly launched [source]",https://web.archive.org/web/20241231073112/https://xbow.com/,"XBOW USA, Inc [source]",Corporation [source],"Incorporation: Delaware, USA (XBOW USA Inc. 3350735) [source]",Unknown,Unknown,None,"The system is given access to source code on a local machine and prompted to find an exploit; it identities strategies, and writes and executes code to test its strategies, e.g [source]",XBOW can observe the outputs of its code execution and observe files on the local machine [source],XBOW can write and execute code and navigate on the local machine [source],Users provide prompts to the system and can observe the system's outputs and code execution [source],Unknown,Unknown,Unknown,Closed source,Unavailable,Closed source,"""We will only make our technology available to trusted customers in the cloud. It is not possible to run XBOW as a standalone application outside our control."" [source]",Unknown,"XBOW is not currently available to external users, and will only be made available to 'trusted customers' [source]","Passes 75 percent of assorted web benchmarks including PortSwigger, PentesterLab, and novel ones [source]; list of all benchmarks available [source]","Various demos/example outputs, e.g [source]",None,None,None,None,None,Not available to external users [source],
OpenWebVoyager,https://arxiv.org/abs/2410.19609,"OpenWebVoyager is an agent for accomplishing open-ended tasks on the internet. This agent supports multimodal observations, and requires minimal human guidance.","Designed to handle more complicated web scenarios, including dealing with multimodal input and sparse supervision. Standard internet interaction benchmarks are text-only: this work aims to extend this.","Paper arXived Oct 25, 2024, and GitHub code initial commit Oct 21, 2024 [source]",https://web.archive.org/web/20250115070638/https://github.com/MinorJerry/OpenWebVoyager/tree/main,Zhejiang University (et al.) [source],"Academic Institution, Corporation",China [source],None,"Uses WebVoyager [source] (which is based on GPT-4o) to obtain imitation data for training multimodal web navigation, and uses GPT-4o directly to evaluate correctness of web trajectories in subsequent self-improvement loops. Agent itself build using Idefics2 [source]",None,"System initially bootstraps basic web browsing capability from imitating a SOTA web browsing agent WebVoyager [source], and then explores real-world web environments in an open-ended way, and the successes are used to augment the initial IL dataset to further improve training.","The 3 past screenshots, and the accessibility tree of the current webpage. ","Can navigate web pages autonomously, and interact with the user (i.e. the agent can click, input, scroll, go back, restart, wait, and provide an answer to the user).","The user can provide the system a query in natural language, but since the reward signal grounded in GPT-4o, unclear how the model grounds it (and, they self-report the system often hallucinates). ",Unknown,Available [source],Available [source],Available [source],Available [source],Available [source],Unknown,Unknown,None,Mind2Web (20%)  [source],See Figure 3 of paper [source],None,None,None,None,None,7 forks and 65 starts on GitHub [source],
AutoGLM,https://arxiv.org/abs/2411.00820v1,"AutoGLM is an agent ""for autonomous control of digital devices through Graphical User Interfaces (GUIs)"". It focuses particularly on web browser and phone GUIs [source]","The system is for operating GUIs, mainly across web browsing and Android environments.","Arxiv submitted on October 28, 2023 [source]",https://perma.cc/497J-2DZL,Zhipu AI [source],Unknown,China [source],Unknown,An internal version of ChatGLM3 (not the open source ChatGLM3) [source],None,Unknown,The screen information displayed on a user's device.,Touch events (as on phone) and mouse/keyboard events (as in a web browser),"A Chrome extension where users write prompts to perform operations on the browser [source]. On a phone, users can provide prompts via text or audio to complete tasks [source]",Unknown,Unknown,They use data from Android-Lab [source],Closed source,Unavailable,Closed source,Unknown,Unknown,None,"VAB-WebArena-Lite (55.2%), OpenTable (96.2%), AndroidLab (VAB-Mobile) (36.2%) [source]","Videos [source], 89.7% success rate ""on common tasks in popular Chinese APPs"" [source]",None,None,None,None,"Chrome, Android [source], and browsers via chromium kernels",None,